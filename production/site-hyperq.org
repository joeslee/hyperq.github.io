# -*- o-blog-local-site: "~/Sites/hyperq"; o-blog-out-dir: "~/git/hyperq\.github\.io"; o-blog-local-url-index: "http://127.0.0.1/~tonyday/hyperq/index.html"; -*-
#+Language: en
#+TITLE: hyperq
#+DESCRIPTION: hyperq project
#+AUTHOR: Tony Day
#+STARTUP: logdone
#+OPTIONS: H:nil num:nil toc:nil \n:nil @:t ::t |:t ^:t f:t TeX:t tags:nil author:nil
#+COLUMNS: %25ITEM %30tangle %5blog %5top %15PAGE
#+LATEX: t

#+PROPERTY: tangle no
#+PROPERTY: session *R*
#+PROPERTY: comments link
#+PROPERTY: noarchive t

* Introductions
Various intro channels: repo README.md, about.html page and index.html
landing page.

The about page is a conversion of the README.md file using pandoc (which has
to be done manually - kludgy I know).

** README.md
:PROPERTIES:
:tangle:   README.md
:END:

#+begin_src markdown
  <img src="http://hyperq.github.io/assets/hyper-blue.png" width="100px">
  
  [Main Site](http://hyperq.github.io)
  
  hyperq is a small project with a big ambition. We aim to build the worlds best
  algorithmic trading platform using the best off-the-shelf open source
  technology stack to be found.
  
  ## Quick start
  
  There is no quick start for hyperq yet (help us create one!). The easiest way to
  get up to speed is to read the project [blog](http://hyperq.github.io/blog). If
  you're interested in contributing to development or find a logic bug, then
  fork me with:
  
  ```
  $ git clone https://github.com/hyperq/hyperq.git
  ```
  
  ## News
  
  25 April, 2013
  
  Happy Birthday, hyperq!  Zero today!
  
  23 April, 2013
  
  A discussion group has been formed at [hft-discuss](https://groups.google.com/forum/?hl=en&fromgroups#!forum/hft-discuss)
  
  Some near-term goals include:
  - moving the project to an organization basis (which will involve a name
    change (watch this space).
  - installing a modern communication toolkit
  - exploring automated deployment and build options
  - discussing the relative strengths and weaknesses of python versus R, as they
    apply to the hft problem domain.
  
  ## Bug tracker
  
  Have a bug or a feature request? [Please open a new issue](https://github.com/hyperq/hyperq/issues). 
  
  ## Community
  
  Join the discussions at [hft-discuss](https://groups.google.com/forum/?hl=en&fromgroups#!forum/hft-discuss)
  
  Hyperq is sponsored by [Scarce Capital](http://scarcecapital.com). Follow
  [@scarcecapital on Twitter](http://twitter.com/scarcecapital).
  
  Read, subscribe (and contribute!) to the [The hyperq Blog](http://hyperq.github.io).
  
  ## Contributing
  
  Please submit all pull requests against the master branch.
  
  Thanks!
  
  ## Core Contributors
  
  **Tony Day**
  
  + [http://twitter.com/tonyday567](http://twitter.com/tonyday567)
  + [http://github.com/tonyday567](http://github.com/tonyday567)
  
  
  ## Copyright and license
  
  Copyright 2013 hyperq.
  
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this work except in compliance with the License.
  You may obtain a copy of the License in the LICENSE file, or at:
  
    [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)
  
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
#+end_src


** Welcome to hyperq!
CLOSED: [2013-04-11 Thu 15:47]
:PROPERTIES:
:PAGE:     about.html
:template: about.html
:END:


[[http://hyperq.github.io][Blog]]

hyperq is a small project with a big ambition. We aim to build the worlds
best algorithmic trading platform using the best off-the-shelf open
source technology stack to be found.


*** Quick start

There is no quick start for hyperq. The easiest way to get up to speed is
to read the project [[http://hyperq.github.io][blog]]. If you're
interested in contributing to development or find a logic bug, then fork
me with:

#+BEGIN_EXAMPLE
    $ git clone https://github.com/hyperq/hyperq.git
#+END_EXAMPLE

*** Bug tracker

Have a bug or a feature request?
[[https://github.com/hyperq/hyperq/issues][Please open a new issue]].

*** Community

Hyperq is sponsored by [[http://scarcecapital.com][Scarce Capital]]. Follow [[http://twitter.com/scarcecapital][@scarcecapital on Twitter]].

Read, subscribe (and contribute!) to the
[[http://hyperq.github.io][the hyperq blog]].

*** Contributing

Please submit all pull requests against the master branch.

Thanks!

*** Core Contributors

*Tony Day*

-  [[http://twitter.com/tonyday567]]
-  [[http://github.com/tonyday567]]

*** Copyright and license

Copyright 2013 Scarce Capital.

Licensed under the Apache License, Version 2.0 (the "License"); you may
not use this work except in compliance with the License. You may obtain
a copy of the License in the LICENSE file, or at:

[[http://www.apache.org/licenses/LICENSE-2.0]]

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


** hyperq
:PROPERTIES:
  :PAGE:     index.html
  :TEMPLATE: plain.html
  :END:

#+begin_html
  <div class="hero-unit">
    <div class="row-fluid">
      <div class="span6">
	<h1>hyperq</h1>
	<br>
	<p>High Profit Algorithms</p>
    <p>High Performance Software</p>
	<p>Quirky, Quick and Quanty</p>
	<p>
	  using <a href="http://www.haskell.org/haskellwiki/Haskell">haskell</a>,<a href="http://www.r-project.org">R</a> and <a href="http://www.gnu.org/software/emacs/">emacs</a> <a href="http://orgmode.org">org-mode</a>.
	</p>
	<p>
	  open-source and on a budget
	</p>
	<a class="btn btn-large hero-button" href="http://eepurl.com/xZDev">email feed &raquo;</a>
    <br>
        <a href="https://twitter.com/hyperq_" class="twitter-follow-button" data-show-count="false">Follow @hyperq_</a>
        <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
        <br>
      </div>


      <div class="span6">
	  <a class="image-link" href="{lisp}(ob:path-to-root){/lisp}/{lisp}(ob:post-htmlfile (ob-get-post-by-title POSTS "Trading: a hacker approach?")){/lisp}">
	    <span class="tooltip">
	      Trading: a hacker approach?
	    </span>
	    <img class="hero-chart" src="<lisp>(ob:path-to-root)</lisp>/<lisp>(ob:blog-assets-dir BLOG)</lisp>/ats_features_diagram.svg" alt="design">
	  </a>
      </div>
   <div class="span6">
<pre class="src src-haskell"><span class="org-function-name">trade</span> <span class="org-variable-name">::</span> [<span class="org-type">MarketData</span> a] <span class="org-variable-name">-&gt;</span> <span class="org-type">Book</span> b <span class="org-variable-name">-&gt;</span> <span class="org-type">IO</span> [<span class="org-type">Order</span> b]
<span class="org-function-name">main</span> <span class="org-variable-name">=</span> forever <span class="org-variable-name">.</span> <span class="org-keyword">do</span> <span class="org-variable-name">.</span> trade
</pre>
</div>

      <div class="span12">

	<p><code>$ git clone https://github.com/hyperq/hyperq.git</code></p>
      </div>
    </div>
  </div>

#+end_html


** Project News April 23,2013
CLOSED: [2013-04-23 Tue 14:44]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-23 Tue 14:44]
:END:
:PROPERTIES:
:blog:     t
:END:

  A discussion group has been formed at [[https://groups.google.com/forum/?hl%3Den&fromgroups#!forum/hft-discuss][hft-discuss]]
  
  Some near-term goals include:
  - moving the project to an organization basis (which will involve a name
    change (watch this space).
  - installing a modern communication toolkit
  - exploring automated deployment and build options
  - discussing the relative strengths and weaknesses of python versus R, as they
    apply to the problem domain.


** hyperq
CLOSED: [2013-04-25 Thu 17:55]
:PROPERTIES:
:blog:     t
:END:

Our little project just got a little bit bigger today and is moving away from
the personal repository to a shiny new open-source organization called hyperq.
We're busy underneath hoods tinkering with link changes, repo structures and
the usual stuff, and apologies in advance if service gets interupted.

*** What does hyperq stand for?

Lots of things.  The name was the end result of thinking of many, many cool
names to do with HFT, machine learning, algorithms and trading, all of which
turned out to be taken already on github.

The project is all about achieving high profits (hy-p) by building high
performance software (hy-per) for High-Frequency Trading (the hy, I suppose).
And it's about putting the q back in quant when it comes to thinking about
low-latency. It's going to be both (q)uick and smart.

But to do all of that we need help (and a touch of patience). A tweet of
encouragement is nice to get, a short critique of our repos even nicer.
Before you know it, you'll be wanting to be a hyperq_t and join in the fun.
  


* technology stack
** project design choices

There are several design features that drive hyperq and we hope offer
a comparative advantage:

- *HFT on a budget*: Development is focused on using the cheapest technologies
  where no free option exists.
- *literate programming documentation*: markets are complex and so is large-scale project
  code. The heart of this project is experimentation with how to go about
  large-scale project formulation using a literate programming ethic.
- *fast AND smart*: much HFT trading is about pure speed - being the first to
  react to an obvious mis-pricing, front-running a lazy or half-hidden market
  order are two obvious examples. We believe there is a gap in the market that
  can be exploited between pure-speed HFT and human-intervention algo trading
  that the project is all about.
- *meta-algo*: the entire system and process is an algorithm to be searched,
  optimised and refactored.
- *open source*: hyperq is open source purely and simply, licenced
  under the generous Apache system.
- *modern toolkit*: the project is oriented towards using higher-level
  languages and concepts for rapid, robust development. Together with a
  literate programming style, this translates to using the right tool for the
  right job and making less compromises with the goals of the project.




The world of high frequency trading is a broad church of opinion,
technology, ideas and motivations. hyperq is currently being developed using many
different tools.

** [[http://www.haskell.org/haskellwiki/Haskell][haskell]]

- re-write with haskell pros and cons

The candidate solution to the design criteria is to use haskell for the system coding:

- haskell has already been used for this purpose :  http://www.starling-software.com/misc/icfp-2009-cjs.pdf
- concurrency is handled quite gracefully:
  http://www.haskell.org/haskellwiki/Concurrency_demos
  http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/beautiful.pdf
- speed is an issue but haskell can often get to within a factor of two versus
  C code. Haskell also plays nicely with other languages so there is room for
  hand-crafting critical sections in C.
- other solutions had issues. Erlang is too wierd and slow, node.js leads to
  callback hell; C, C++ and Java lead to a much larger code base.

** [[http://www.gnu.org/software/emacs/][emacs]],
[[http://orgmode.org][org-mode]] and
[[http://en.wikipedia.org/wiki/Literate_programming][literate
programming]]

[[https://github.com/hyperq/hyperq/blob/master/hyperq.org][hyperq.org]] is
the nerve center of active development and contains just about all the
important code, research notes and design tools being used.

The project makes heavy use of
[[http://orgmode.org/worg/org-contrib/babel/][babel]] to pick and mix
between coding environments and languages, whilst still remaining
[[http://www.haskell.org/haskellwiki/Literate_programming][literate]]:

#+BEGIN_QUOTE
  The main idea is to regard a program as a communication to human
  beings rather than as a set of instructions to a computer. ~ Knuth
#+END_QUOTE

Similarly, a project such as hyperq is as much about communication between
human beings as it is about maintenance of source code.

** [[http://www.r-project.org][R]]

R is a strongly functional but imperative language being used for rapid
development and research of hft and algo ideas as they arise. Most
everything that you can think of (databases, broker interfaces,
statistical analysis, visualization) has an R package ready to get you
up and going in 5 minutes.

** [[http://www.interactivebrokers.com/en/main.php][Interactive
Brokers]]

Eventually, the hft schema will be broker independent but during the development
phase IB is the test case. Interactive has the most mature API that
works out of the box and a demo account so that hyperq can come pre-plumbed
and (eventually) the project can also run out of the box.

Interactive Brokers consolidates tick data into 0.3 second time slices
so it isn't appropriate for low-latency work.

** [[http://www.iqfeed.net][iqfeed]]

Just because it's open-source doesn't mean that it's cost free. iqfeed
has been chosen as an initial data feed to base project R&D efforts on.
iqfeed costs dollars but the software can be downloaded for free and a
demo version allows live data to flow with a lag.

A useful way to support the hyperq is to let DTN know if you decide
to purshase iqfeed.





** Open Source Options
*** zipline
CLOSED: [2013-04-23 Tue 15:11]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-23 Tue 15:11]
:END:
:PROPERTIES:
:blog:     t
:AUTHOR:   Chris Mahon
:END:

Zipline is an open-source project that is a candidate for collaboration and
other synergies with hyperq.

Zipline is the open-source backtesting engine (written in Python) underlying
Quantopian, an online trading systems development IDE (again, Python-based)
and community. Quantopian offers 10 years of 1min data for US stocks and the
backtesting IDE for free but they plan to generate income from a future
facility for automating live trade execution.

https://www.quantopian.com/faq
https://www.quantopian.com/help
https://github.com/quantopian/zipline/blob/master/README.md

To use the zipline engine, you basically create a Python class that subclasses
zipline's TradingAlgorithm base class and implement 2 methods, one to
initialize() data at the start of strategy execution and and another,
handle_data(), to react to data events (e.g. buy on MA crossover) which are
typically OHLCV bars but could be of other types such as ticks, corporate
actions, etc. A strategy is executed by calling the trading algorithm's run()
method and passing in a data source. Various backtesting metrics are returned
by the run() method.

Base functionality includes:
- Loaders for retrieving data from local files, online sources such as yahoo
  (no live data feeds - data is downloaded in its entirety prior to strategy
  execution) and the US treasury website for risk-free rates used in metrics
  such as the Sharpe ratio

- Multiple symbols can be tracked within a dataset and referenced in a single
  strategy execution

- A few basic technical indicators out of the box such as simple moving
  average, returns, standard deviation of prices and VWAP

- Data is provisioned to the strategy via events on the backtest timeline to
  reflect live trading and reduce the scope for future data snooping

- Only market orders supported - no doubt limit and stop orders are planned as
  a future enhancement

- Trade execution includes 2 slippage models and a basic commission calc

- No order book model

- A portfolio of holdings can be accumulated and tracked

- A variety of metrics are calculated cumulatively and per-period (daily basis
  if intraday bars are used) e.g. strategy and benchmark (defaults to SPX)
  returns, drawdown, sharpe ratio, sortino ratio, etc

- Mainly focused on stocks for the time being e.g. the concept of end of
  trading day seems to be important vs day continuous trading of forex and
  overnight trading of futures, no margin trading

- Of course there's plenty of scope to extend the base functionality through
  custom code - this may be more constrained in the IDE than in a local
  development environment.

- Trading system optimization isn't supported yet but appears to be an active
  area of research for a future enhancement. One of the developers gave a talk
  at the recent PyData conference about using IPython Parallel, StarCluster,
  etc. to spawn multiple parallel workers in the cloud as well as covering
  techniques to efficiently search the parameter space including a Bayesian
  approach for identifying high probability regions to maximize the objective
  function.

  http://vimeo.com/63273425


The gist below is a first pass through the codebase tracking the call stack
and classes/objects involved in the execution of the trading algorithm run()
method for the supplied dual moving average sample strategy and recorded the
main features in a single file. This is useful to quickly refresh memory
of the code flow rather than wading though multiple source files.

https://gist.github.com/cmahon/e72e0e08de2986c2180d




** other system components and projects

[[http://www.activequant.org][ActiveQuant]]
[[https://github.com/Neverlord/libcppa][libcppa]]
[[http://esper.codehaus.org][esper]]
[[http://triceps.sourceforge.net][triceps]]
[[http://code.google.com/p/cep-trader/][cep-trader]]
[[http://algo-trader.googlecode.com][algo-trader]]
[[http://code.google.com/p/tradelink/][tradelink]]
[[http://artstkmkt.sourceforge.net/][ASM]]
[[https://github.com/penberg/libtrading][libtrading]]
[[https://github.com/dakka/fix8][fix]]
[[http://tradexoft.wordpress.com/][tradexoft]]

https://github.com/andykent/river
  I like the idea of pushing data through queries (rather than
  queries through data).

http://www.quickfixengine.org/quickfix/doc/html/about.html
  major broker API



* runtime design

The solution space for a highish-frequency, automated, algorithmic trading
system is wide. Most solutions out there are propietary, expensive and
expansive.

** Trading: a hacker approach?
CLOSED: [2013-06-16 Sun 08:19]
:LOGBOOK:
- State "DONE"       from ""           [2013-06-16 Sun 08:19]
:END:
:PROPERTIES:
:blog:     t
:END:

#+begin_quote
You are startled by the sound of an alarm. It is followed by an urgent voice
which warns that the Arcada has been boarded by unknown intruders. It ends
abruptly. \\
> \\
[[http://sarien.net/spacequest#anotherhallway][start of Space Quest I]]
#+end_quote

Most hackers involved in the world of trading enter from the technology side
of the business. And there's two main gateways are via trader enhancement or
trader replacement. Making traders smarter and faster using technology is one
well worn road. There's lots of room to streamline the human trading process:
automation of regular tasks, expansion of back-testing capabilities and easy
gains to be had in better trader dashboards to get information when and where
needed.

Trader replacement is a little harder but also hackable. There's plenty of
tricks out there to shave a few msecs of computation and execution time, and
bringing bigdata testing and conversion of a sometimes fuzzy human rule-set
into a more rigorous computational exercise.

Either way, a trading runtime ends up paving the cow-paths of institutional
finance which look somewhat like this:

#+html: <img src="<lisp>(ob:path-to-root)</lisp>/<lisp>(ob:blog-assets-dir BLOG)</lisp>/ats_features_diagram.svg" alt="design" width="100%">

Over at [[http://hyperq.github.io][hyperq]], we've been thinking about the above diagram and how to get
together a decent trading runtime. Now when a wildly ambitious objective meets
a meager resource base you have two options:

- go on a [[file:img/ENOxd.jpg][kamikaze]] death march
- take the team on a [[http://www.qt.com.au/news/abolish-fringe-benefits-tax-bring-back-long-lunch-/1895360/][long lunch]] and redefine

Since this is all open source, our long lunch redefinitional musings led us to
computer sciencing the bejesus out of the trading problem domain.

Here's the alternative design specification document we wrote on one of the
drink coasters:

#+BEGIN_SRC haskell 
trade :: [MarketData a] -> Book b -> IO [Order b]
main = forever . do . trade
#+END_SRC

Having just cut 3 months out of our critical path we even had time for some
Zork:

#+begin_quote
You are in an open field west of a big white house with a boarded
front door.
There is a small mailbox here.\\
> \\
[[http://thcnet.net/error/index.php][Zork]]
#+end_quote

Long lunch over and the new specs still seem sweet. Some immediate ideas:

- the concept of market data becomes naturally abstractable.  Data can
  include multiple sources, news flow or whatever universe observation you
  can think of. Do you need the Market prefix? 
- there is an immediate reminder of real world interaction with the IO monad.
- subsequent functions scan more easily and can be categorized - often as a
  matter of taste.  For example, a complex event process (CEP), a fashionable
  big deal in trading system circles, seems logically to have this type:

  #+begin_src haskell
  cep :: [MarketData a] -> MarketData a
  #+end_src

  Whether to put this prior to or inside the trade function then becomes a
  matter of taste.

- Should the input be [Maybe MarketData a]? This puts the real world
  likelihood of the data feed being down front and center, rather than
  designing a system assuming an idealized world and then panicking when
  something breaks.

More generally, a more hacker approach leads you away from bigdata phd
solutions that dominate hft and algorithmic trading and towards the important,
small and obvious stuff (that may not lend themselves to a phd dissertation).
The market is closed (unexpectedly) - I better not try and trade, or I had
better try and trade elsewhere given the sorry state of Book a. Gee, there's a
lot of volatility out and about today - is it a big news day? The last news
piece of note was a facebook announcement. Wow, facebook really tanked but but
zygna didn't - what gives there? Someone must have forgot to turn the market
feed on. etc etc

And I suspect this approach leads further to a big, big gap in the market.
Imagine on a busy day in the market you could slow down time. The e-mini
suddenly drops by 1% in the space of a few heartbeats. What just happened?
Rewind the video tape and look more carefully at the last few minutes. Look
back at the news-flow over the last 10 minutes and look for keywords. Check
other markets - are they all tanking or is it just a local event? Or did some
human just enter an extra zero or three again?

If you can do all of that in a few seconds your process is way ahead of the
competition. The HFT guys have already panicked and run away to hide behind
their statistical order flow models. Algorithmic trades are pinging their stop
loss instructions blindly creating what may be a forecastable trend.
Meanwhile, discretionary day traders have just noticed a small section of one
of their screen is flashing red...

In this zone, a hacker trader with a hacker-like trading process can find all
sorts of edges and market tells.

So do we want our trading process to look and feel like a big finance
organizational structure? Or should for hyperq to have a Roger Wilco attitude:

#+begin_quote
Anyway, I aborted the launch and jetted out of there in an escape pod. I
crawled into the sleep chamber, and the next thing I knew, I woke up in a
trash freighter! Yeah, things didn't look too good, but I blasted out of the
freighter in an old jalopy I resurrected from the rubble.
~ [[http://spacequest.wikia.com/wiki/Roger's_Dialogue][Roger Wilco]]
#+end_quote

Much more fun than a death march to pave the cow-paths.

** Design Experimentation                                            :design:
CLOSED: [2013-04-11 Thu 16:46]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-11 Thu 16:46]
:END:
:PROPERTIES:
:blog:     t
:top:      t
:END:
 
hyperq is an experiment in runtime design and, as such, there is a need for flexibility
in the top-down design of the system. To achieve this, the overall design is
first being modelled using graphviz.  The current candidate system looks like this:

#+html: <img class="hero-chart" src="<lisp>(ob:path-to-root)</lisp>/<lisp>(ob:blog-assets-dir BLOG)</lisp>/candidate.svg" alt="design" width="100%">

- blue boxes represent individual components of the system
- other colors represent external systems and data sources
- each edge of the chart represents a messaging sytem requirement
- there are two main one-way message passing routines that probably
  need to be very very fast (blue lines)
- there is one read from database and one write to database (red lines)
- every component registers to an observer component that records system
  state and dynamics (grey dotted).

The components have been grouped into several clusters:

- market data: representing trade data, order book and news information
  flowing from outside the sytem to a local data node.
- broker data: representing communication with trading mechanisms
- onwire: components that are "in the event stream".  This is motivated by
  the specifications and documentation of the disruptor which argues that a
  single thread "wheel" is the best way to enable fast processing of market
  data into trading orders.
- offwire: this represents algorithms and processing that are not on the
  single-thread process.  The motivation here is to test the hypothesis in
  the disruptor argument.

There are several ideas that are being tested:

- that the entire system should be the subject of search and optimisation,
  rather than componentry.  One example of this is separation of complex
  event definitions from the statistical analysis once events are defined.
- there is a focus on automation and machine learning.  As such there is no
  place for human interaction.  In particular, no visualization is required.
- messaging between components can be the same general process.  The
  components can also be tested in exactly the same way (such as speed and
  robustness testing)

And here's the dot code:

#+begin_src dot :file candidate.svg :cmdline -Kdot -Tpng :exports code
digraph G {
	node [label="\N"];
	node [style=filled, color="#1f3950",fontcolor="#eeeeee",shape=box];
	subgraph cluster_market_data {
		graph [label="market data", color="#909090"];
		exchange [shape=egg,color="#ff111111",fontcolor="#101010",label="exchanges"];
		aggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="data stream"];
		localport [label="local node"];
		exchange -> aggregator [dir=none];
		aggregator -> localport [dir=both];
	}
	subgraph cluster_offwire {
		graph [label="offwire",
			color="#909090"];
		offwirealgo [label="offline algo"];
		observer;
		databases;
		observer -> databases [color=red,label="write",fontcolor=red];
	}
	subgraph cluster_onwire {
		graph [label="onwire",
			color="#909090"];
		node [style=filled];
		disruptor [label="event server"];
		eventalgo [label="algo"];
		controller;
		controller -> eventalgo [color="#aaaaaa",dir=both]
		disruptor -> listener;
		disruptor -> eventalgo;
		disruptor -> controller;
		controller -> disruptor [color="#0080ff"];
	}
	subgraph cluster_broker {
		graph [label="broker data",
			color="#909090"];
		broker [shape=egg,color="#ff111111",fontcolor="#101010",label="brokers"];
		brokeraggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="aggregation"];
		broker -> brokeraggregator [dir=none];
		brokeraggregator -> trader [dir=both];
	}
	localport -> observer [color="#aaaaaa",style=dotted];
	controller -> localport [color="#aaaaaa"];
	localport -> disruptor [color="#0080ff"];
	listener -> observer [color="#aaaaaa",style=dotted];
	controller -> observer [color="#aaaaaa",style=dotted];
	controller -> trader [color="#aaaaaa",dir=both];
	controller -> offwirealgo [color="#aaaaaa",dir=both];
	databases -> offwirealgo [color=red,label="read",fontcolor=red];
	trader -> observer [color="#aaaaaa",style=dotted];
	eventalgo -> observer [color="#aaaaaa",style=dotted];
	offwirealgo -> observer [color="#aaaaaa",style=dotted];
}
#+end_src
** design -> code automation

*** candidate svg dot file

#+begin_src dot :file img/candidate.svg :cmdline -Kdot -Tsvg :exports both
digraph G {
        node [label="\N"];
        node [style=filled, color="#1f3950",fontcolor="#eeeeee",shape=box]; 
        subgraph cluster_market_data {
                graph [label="market data", color="#909090"];
                exchange [shape=egg,color="#ff111111",fontcolor="#101010",label="exchanges"];
                aggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="data stream"];
                localport [label="local node"];
                exchange -> aggregator [dir=none];
                aggregator -> localport [dir=both];
        }
        subgraph cluster_offwire {
                graph [label="offwire",
                        color="#909090"];
                offwirealgo [label="offline algo"];
                observer;
                databases;
                observer -> databases [color=red,label="write",fontcolor=red];
        }
        subgraph cluster_onwire {
                graph [label="onwire",
                        color="#909090"];
                node [style=filled];
                disruptor [label="event server"];
                eventalgo [label="algo"];
                controller;
                controller -> eventalgo [color="#aaaaaa",dir=both]
                disruptor -> listener;
                disruptor -> eventalgo;
                disruptor -> controller;
                controller -> disruptor [color="#0080ff"];
        }
        subgraph cluster_broker {
                graph [label="broker data",
                        color="#909090"];
                broker [shape=egg,color="#ff111111",fontcolor="#101010",label="brokers"];
                brokeraggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="aggregation"];
                broker -> brokeraggregator [dir=none];
                brokeraggregator -> trader [dir=both];
        }
        localport -> observer [color="#aaaaaa",style=dotted];
        controller -> localport [color="#aaaaaa"];
        localport -> disruptor [color="#0080ff"];
        listener -> observer [color="#aaaaaa",style=dotted];
        controller -> observer [color="#aaaaaa",style=dotted];
        controller -> trader [color="#aaaaaa",dir=both];
        controller -> offwirealgo [color="#aaaaaa",dir=both];
        databases -> offwirealgo [color=red,label="read",fontcolor=red];
        trader -> observer [color="#aaaaaa",style=dotted];
        eventalgo -> observer [color="#aaaaaa",style=dotted];
        offwirealgo -> observer [color="#aaaaaa",style=dotted];
}
#+end_src

#+results:
[[file:img/candidate.svg]]


- blue boxes represent individual components of the system
- other colors represent external systems and data sources
- each edge of the chart represents a messaging sytem requirement
- there are two main one-way message passing routines that probably
  need to be very very fast (blue lines)
- there is one read from database and one write to database (red lines) 
- every component registers to an observer component that records system
  state and dynamics (grey dotted).


The components have been grouped into several clusters:

- market data: representing trade data, order book and news information
  flowing from outside the sytem to a local data node.
- broker data: representing communication with trading mechanisms
- onwire: components that are "in the event stream".  This is motivated by
  the specifications and documentation of the disruptor which argues that a
  single thread "wheel" is the best way to enable fast processing of market
  data into trading orders.
- offwire: this represents algorithms and processing that are not on the
  single-thread process.  The motivation here is to test the hypothesis in
  the disruptor argument.

There are several ideas that are being tested:

- that the entire system should be the subject of search and optimisation,
  rather than componentry.  One example of this is separation of complex
  event definitions from the statistical analysis once events are defined.
- there is a focus on automation and machine learning.  As such there is no
  place for human interaction.  In particular, no visualization is required. 
- messaging between components can be the same general process.  The
  components can also be tested in exactly the same way (such as speed and
  robustness testing)




*** haskell interaction

Via haskell, the dot chart can be the specifications for an actual system as well as a
representation. And via svg technology, the picture can also be modified to
be a reporting front-end in a production environment.

So, a new picture generates a new system with potentially new components
(nodes) and messaging requirements (edges).

*** edges
:PROPERTIES:
:blog:
:END:

#+begin_src haskell :results value
import ControllerTest
g <- importDotFile "../candidate.dot"
edgeList g
#+end_src

| exchange         | aggregator       |
| aggregator       | localport        |
| observer         | databases        |
| controller       | eventalgo        |
| disruptor        | listener         |
| disruptor        | eventalgo        |
| disruptor        | controller       |
| controller       | disruptor        |
| broker           | brokeraggregator |
| brokeraggregator | trader           |
| localport        | observer         |
| controller       | localport        |
| localport        | disruptor        |
| listener         | observer         |
| controller       | observer         |
| controller       | trader           |
| controller       | offwirealgo      |
| databases        | offwirealgo      |
| trader           | observer         |
| eventalgo        | observer         |
| offwirealgo      | observer         |


*** nodes

#+begin_src haskell
import ControllerTest
import Data.List
g <- importDotFile "../dot/candidate.dot"
map (\x -> [x]) $ nodeList g
#+end_src

| aggregator       |
| broker           |
| brokeraggregator |
| controller       |
| databases        |
| disruptor        |
| eventalgo        |
| exchange         |
| listener         |
| localport        |
| observer         |
| offwirealgo      |
| trader           |





* event feed
** feed component
:PROPERTIES:
:tangle:   dot/iqcontroller.dot
:END:

#+begin_src dot :file img/iqtest.png :cmdline -Kdot -Tpng :exports both
  digraph G {
          node [label="\N"];
          node [style=filled, color="#1f3950",fontcolor="#eeeeee",shape=box];
          
          subgraph cluster_feed {
                  graph [label="feed control",
                          color="#909090"];
                  node [style=filled];
                  controller;
                  admin;
                  controller -> admin [color="#aaaaaa",dir=both]
          }
          subgraph cluster_iqport {
                  graph [label="iqport",
                          color="#909090"];
                  adminport [shape=egg,color="#cc11cc22",fontcolor="#101010"];
                  marketport [shape=egg,color="#cc11cc22",fontcolor="#101010"];
                  lookupport [shape=egg,color="#cc11cc22",fontcolor="#101010"]; 
          }
          
          admin -> adminport [color="#aaaaaa",dir=both];
          controller -> marketport [color="#aaaaaa", dir=both];
          controller -> lookupport [color="#aaaaaa", dir=both];
          controller -> stdin [color="#aaaaaa", dir=back]
          controller -> stdout [color="#aaaaaa"]
  }
#+end_src

#+results:
[[file:img/iqtest.png]]

** Event Feed Design
CLOSED: [2013-04-30 Tue 10:48]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-30 Tue 10:48]
:END:
:PROPERTIES:
:blog:     t
:top:      t
:END:

The hyperq_team is currently looking at development priorities and, whilst
the votes aren't all in, designing and building the event feed is a hot
favorite.  At the moment, the component is labeled as a market data stream but I'd like
to broaden the definition.  

Concentrating solely on market trades, bids and asks for an individual
security in an effort to fully understand what's happening with price is a
dangerous activity. We think there's a lot to be gained by /always/ accounting
for broad market conditions /before/ you start to do statistical analytics on
a particular security. Before you decide that =goog= is <insert forecast
metaphor here>, you should check first as to whether the overall =market= is
<insert forecast here> as well. If you look really, really closely, you
might find that the =market= starts trending /before/ =goog= does, and you
have yourself an immediate edge.

This statistical arbitrage analogy can be extended beyond market data into
market news flow. As a human, if I see a news item on Google, the very first next
thing I do is look at recent Google price movements. If this could be automated (and it
can) a news flow can trigger a recent order history lookup, a calculation of
'price spikiness', a switch to including the security in the event stream.
With 1000 news flow items a day we're going to find a handful of fat pitch
opportunities.

Even more broadly, social media news flow is evolving rapidly and could be a
great source of automated idea generation.  A recent example, and fellow
startup is http://www.ftsee.com/.  Machine-readable news flow, json-based API,
easy ticker lookups - it's going to be a joy hooking this up after the pain
involved with old and creaking market data feed APIs.

So I wonder what we'll find when we crunch the correlations between intra-day
returns and intra-day ftsee sentiment?  If we do find something I'm not sure we'll
be telling unless you're a contributor ;)

The concept of an event stream can also be extended towards events that happen
inside the hyperq internals. If the internet goes down or (more likely) my son
hacks into the dedicated line to steam the latest 30gig TF2 upgrade, then
that's an event much more important than any external data streaming in (or
not streaming in). Stop calculating the finer nuances of market stochastics,
please, and start prepping for panic trading on link resumption.  Another
example is the risk management component - knowing that near-term P&L is
dependent on a few key positions should mean that event processing needs to
be prioritized for these securities.  And the best and fastest way to get at
event processors may well be a message sent via the event stream.

So we're thinking about an event stream very broadly and including concepts
other than market data and even external data. I think we can think this way
because of our commitment to a modern multi-toolkit over the pure speed java
approach. You can do things in haskell that can't be imagined in imperative
languages, and you can create things with a multi-tool structure that can't
be created in any single language.



** dev notes
- things that need to be (step 1)
  - a logon process
  - a delay if we are logging on
  - an admin listener and logger
  - an admin writer
  - an IO to start the process
  - an IO to stop the process
  - a level1 event feed listener and logger
  - a level1 event feed writer
  - a level2 event feed listener and logger
  - a level2 event feed writer
  - a history lookerupperer writer
  - a history lookerupperer listener and logger
  - code that decides what is written to each port
  - a place to log to + a stamp
  - a Controller (a big ? on this) that isn't main
- abstractions
  - these are all threads
  - portWriter
  - portReader (portListener)
  - portLogger (is this a specialisation of portReader)
- state information
  - is iqconnect up and are we logged on?
    - interrogation of system threads?
    - is there anything spitting out of admin port?
  - which processes are running?  Do we need to know this?
    - promises baked in to portWriter etc (?)
    - do processes need to register with a Controller?
- things that need to be (step 2):

  - login
    - starts (restarts) iqconnect
    - is encapsulated entirely within the manager(?)  In other words,
      it will only be invoked by the manager when admin port throws
      an error.

  - admin
    - an admin portReader that
    - decides if iqconnect is logged on (how? - via port error)
    - relogs if the connection is bung (up to a limit)
      - delays other threads until relogged
    - listens forever
    - decides when to stop
    - decides when to start
    - stops and starts other processes (?)

  - a main that
    - starts admin
    - starts ALL portReaders
    - starts the worker pool
    - listens for a stop instruction (IO)
    - stops everything
    - decides what to write to ports
    - listens on STDIN

  - a feed portReader that
    - stays connected to the feed port
    - sleeps if port error
    - logs all STDOUT
    - same for ALL ports

  - a feed portWriter that
    - sets the protocol
    - changes the ticker list
    - asks for news, fundamentals etc
    - does other stuff? - check iq feed API

  - a history portWriter that
    - requests information (ticker, time range) 

  - an admin portWriter that

  - a lookup portWriter that

  - a worker pool (STM) for all threads that
    - provides a new thread
    - stop/starts threads (?)
    - monitors threads (?)  

- heirarchy (small to big)
    - logon
    - portReader
        - portLogger
    - portWriter
        - historyWriter
        - feedWriter
        - level2Writer
    - workerPool
    - manager (connection management)
    - main




** market feed choices
CLOSED: [2013-04-12 Thu 12:46]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-11 Thu 16:46]
:END:
:PROPERTIES:
:blog:     t
:top:      t
:END:

There is no such thing as live market data for free (please let us know if
this is wrong!).

The closest to free data is the Interactive Brokers feed.  IB consolidate
market data and post every 0.3 seconds however, making it unsuitable for
testing lower-latency ideas.

Initial testing of market data is concentrating on [[http://www.iqfeed.net/][iqfeed]].
- iqfeed is the cheapest "unencumbered" market data feed option
- it can be downloaded for free and a demo account used for testing (data is
  delayed)
- 5.0 has just been released and this includes millisecond resolution for
  both trade and quote times.

Now the bad news:
- iqfeed exists only as windows software
- the process is hardwired to communicate via a tcp connection.
- the feed has a habit of going down several times a day so that there will
  be gaps in the event stream.
- you will need a login id and password to use the software which you get in
  a free trial

*** iqfeed

**** Port comms

There are 4 main communication points to iqfeed:

Level1Port 	5009 	Streaming Level 1 Data and News
Level2Port 	9200 	Streaming Market Depth and NASDAQ Level 2 Data
LookupPort 	9100 	Historical Data, Symbol Lookup, News Lookup, and Chains Lookup information
AdminPort 	9300 	Connection data and management.

More information can be obtained at [[https://www.iqfeed.net/dev/api/docsBeta/Introduction.cfm][DTN IQFeed Developer Area]] or https://www.iqfeed.net/dev/main.cfm
(for a price).

**** Setup info

iqfeed is available for download via
http://www.iqfeed.net/index.cfm?displayaction=support&section=download

Personally, my development environment is on a mac so I need to start and
manage the process via wine.

From the command line:

For the demo product (delayed feed):
#+begin_src sh
wine "Z:\\Users\\tonyday\\wine\\iqfeed\\iqconnect.exe" -product IQFEED_DEMO -version 1
#+end_src

#+begin_src sh
nc localhost 5009
#+end_src

For a live account:
#+begin_src sh
wine "Z:\\Users\\tonyday\\wine\\iqfeed\\iqconnect.exe" ‑product yourproductid ‑version 0.1 ‑login yourlogin ‑password yourpassword -autoconnect -savelogininfo
#+end_src

** Latency Research                                             :research:R:
CLOSED: [2013-04-12 Thu 16:46]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-11 Thu 16:46]
:END:
:PROPERTIES:
:blog:     t
:top:      t
:END:

I collected trade and order ticks for 12 contracts on 14th March from iqfeed,
and timestamped each tick with current system time. There was about 8 million
data points.


*** feed ping latency

Iqfeed sends a ping once a second as part of the stream.

    #+begin_src R
      t = read.csv("data/streamt.txt",header=FALSE,as.is=TRUE)
      pingtime = strptime(t[,3], "%Y%m%d %H:%M:%S")
      stamp = strptime(paste(strftime(pingtime,"%Y%m%d"), t[,1], sep=" "), "%Y%m%d %H:%M:%OS")
      latency = as.double(stamp - pingtime)
      df = data.frame(pingtime=pingtime, latency=latency)
      summary(df)
    #+end_src

    | Min.   :2013-03-14 07:30:57 | Min.   :-0.90665 |
    | 1st Qu.:2013-03-14 17:15:41 | 1st Qu.:-0.01492 |
    | Median :2013-03-15 03:02:15 | Median : 0.14950 |
    | Mean   :2013-03-15 03:01:28 | Mean   : 0.38876 |
    | 3rd Qu.:2013-03-15 12:46:33 | 3rd Qu.: 0.22824 |
    | Max.   :2013-03-15 22:33:24 | Max.   : 7.89887 |
    | NA's   :1                   | NA's   :1        |

    #+begin_src R
    require(ggplot2)
    qplot(data=df, x=pingtime, y=latency)
    ggsave("ping-latency.svg")
    #+end_src

    #+results:

    [[file:assets/ping-latency.png]]

    The simple scatterplot shows many negative values, especially when the
    market is open, and a step jump in the later pings (when no quotes were
    being recorded).  These jumps may be due to changes in my system clock
    (automatic appletime resolutions) or due to a lack of accuracy in the
    iqfeed pings.

    Scatterplots tend to provide dubious visualisation for bigdata, and a new
    package out that helps is [[http://vita.had.co.nz/papers/bigvis.html][bigvis]].

    Bigvis is not yet available at CRAN but can be installed via a github
    repository (see https://github.com/hadley/bigvis for details).

    #+begin_src R
    install.packages("devtools")
    devtools::install_github("bigvis")
    #+end_src

    Bigvis doesn't handle non-numeric data (like time), so rather than
    autopilot, I use ggplot directly.

    #+begin_src R :results file
      require(bigvis)
      require(ggplot2)
      dfn = condense(bin(as.double(df$pingtime),60),bin(df$latency,.1))
      dfg = data.frame(as.POSIXct(dfn[,1],origin="1960-01-01", tz="GMT"),dfn[,2],dfn[,3])
      colnames(dfg) = c("Time","Latency","Count")
      g = ggplot(data=dfg,aes(x=Time,y=Latency))
      g + geom_tile(aes(fill=Count)) + scale_fill_gradient(low="#e5e5e5", high = "#444548") + scale_y_continuous(limits=c(-1,1))
      ggsave("ping-latency-condensed.svg")
   #+end_src

   [[file:assets/ping-latency-condensed.svg]]

   Using the bigvis techniques clarifies a few main issues for further research:
   - there is a step jump near market open where the majority of the pings
     jump from around 250 msecs to -750 msecs. This looks like either a coding
     error or the ping being off by up to a second.
   - during market open (when tick volume is high) ping can vary by a second.



*** disconnects
   Just looking at the ping counts after binning into one minute intervals:

   #+begin_src R
      df.dis = condense(bin(as.double(df$pingtime),60))
      dfg = data.frame(as.POSIXct(df.dis[,1],origin="1960-01-01", tz="GMT"),60-df.dis[,2])
      colnames(dfg) = c("Time","Count")
      g = ggplot(data=dfg,aes(x=Time,y=Count))
      g + geom_line(aes())
      ggsave("disconnects.png")

   #+end_src

   [[file:assets/disconnects.png]]

   iqfeed regularly suffers from disconnects with reconnection occuring within
   a minute.


*** event latency

The ticks for the day were processed into column-data formats using the mmap
package from R (see hyperq.org for the gory details).

From the R database of the one day quote ticks...

- open data
  #+begin_src R

  rm(list = ls())
  require("mmap")
  require("rindex")
  require("plyr")
  require("stringr")
  raw.stream = "streamqh"
  # where the mmap db is located
  db.path = paste("data/",raw.stream,"/",sep="")

  load(paste(db.path,".Rdbinfo",sep=""))
  #m = mmap(main.filename, mode=st)
  stream = NULL
  stream$stamp = mmap(paste(db.path,fields[1],".data",sep=""), mode=double())
  stream$code = mmap(paste(db.path,fields[2],".data",sep=""), mode=char(1))
  stream$symbol = mmap(paste(db.path,fields[3],".data",sep=""), mode=char(ticker.length))
  stream$trade = mmap(paste(db.path,fields[4],".data",sep=""), mode=double())
  stream$vol = mmap(paste(db.path,fields[5],".data",sep=""), mode=integer())
  stream$tradetime = mmap(paste(db.path,fields[6],".data",sep=""), mode=double())
  stream$tradeex = mmap(paste(db.path,fields[7],".data",sep=""), mode=double())
  stream$volex = mmap(paste(db.path,fields[8],".data",sep=""), mode=integer())
  stream$tradetimeex = mmap(paste(db.path,fields[9],".data",sep=""), mode=double())
  stream$voltot = mmap(paste(db.path,fields[10],".data",sep=""), mode=integer())
  stream$bid = mmap(paste(db.path,fields[11],".data",sep=""), mode=double())
  stream$bidvol = mmap(paste(db.path,fields[12],".data",sep=""), mode=integer())
  stream$bidtime = mmap(paste(db.path,fields[13],".data",sep=""), mode=double())
  stream$ask = mmap(paste(db.path,fields[14],".data",sep=""), mode=double())
  stream$askvol = mmap(paste(db.path,fields[15],".data",sep=""), mode=integer())
  stream$asktime = mmap(paste(db.path,fields[16],".data",sep=""), mode=double())
  stream$event = mmap(paste(db.path,fields[17],".data",sep=""), mode=char(12))
  stream$id = mmap(paste(db.path,fields[18],".data",sep=""), mode=integer())

  #+end_src

  #+results:

- Define events and extract relevant times
  #+begin_src R
  n = length(stream$event[])

  tC = grepl("C",stream$event[])
  tO = grepl("O",stream$event[])
  ta = grepl("a",stream$event[])
  tb = grepl("b",stream$event[])
  ta = ta & !(tC | tO)
  tb = tb & !(tC | tO | ta)
  tother = !(ta | tb | tC | tO)

  event.category = (1 * tC) + (2 * tO) + (3 * ta) + (4 * tb) + (5 * tother)

  event.time = (stream$tradetime[] * tC +
	  stream$tradetimeex[] * tO +
	  stream$asktime[] * ta +
	  stream$bidtime[] * tb +
	  stream$tradetime[] * tother)

  event.time.posix = as.POSIXct(event.time,origin="1960-01-01", tz="GMT")
  event.stamp = stream$stamp[]

  event.latency = event.stamp - event.time

  event.df = data.frame(symbol=stream$symbol[],event.category,event.time, event.stamp, event.latency)
  summary(event.df)
  #+end_src

  | @ESM13 :2553308 | Min.   :1.000 | Min.   :1.366e+09 | Min.   :1.366e+09 | Min.   :-85800.76 |
  | @NQM13 :1285545 | 1st Qu.:3.000 | 1st Qu.:1.366e+09 | 1st Qu.:1.366e+09 | 1st Qu.:     0.22 |
  | @YMM13 :1216006 | Median :3.000 | Median :1.366e+09 | Median :1.366e+09 | Median :     0.33 |
  | EBK13  : 917275 | Mean   :3.107 | Mean   :1.366e+09 | Mean   :1.366e+09 | Mean   :   226.44 |
  | @JYM13 : 844995 | 3rd Qu.:4.000 | 3rd Qu.:1.366e+09 | 3rd Qu.:1.366e+09 | 3rd Qu.:   600.22 |
  | EBM13  : 610827 | Max.   :5.000 | Max.   :1.366e+09 | Max.   :1.366e+09 | Max.   :  9818.25 |
  | (Other):1373320 | nil           | nil               | nil               | nil               |

- bigvis manipulations
  #+begin_src R
  require("bigvis")
  require("ggplot2")
  df1 = condense(bin(event.df$event.time,60),bin(event.df$event.latency,0.05))
  df2 = df1[(df1$event.df.event.latency > 0) & (df1$event.df.event.latency < 1),]
  dfg = data.frame(as.POSIXct(df2[,1]+10*60*60,origin="1960-01-01", tz=""),df2[,2],df2[,3])
  colnames(dfg) = c("Time","Latency","Count")
  g = ggplot(data=dfg,aes(x=Time,y=Latency))
  g + geom_tile(aes(fill=Count)) + scale_fill_gradient(low="#e5e5e5", high = "#444548") + scale_y_continuous(limits=c(-1,1))
  ggsave("quote-latency-condensed.svg")

  #+end_src

  #+results:

  [[file:assets/quote-latency-condensed.svg]]

  Unlike the iqfeed ping, there is a consistent latency pattern when comparing
  market stamp and local system stamp, with no spurious negative values.
  Latency values from 9am to 4pm (regular market open) at 500 millsecs are
  common and may be due to TCP issues (dropped packets say).

- symbols

  #+begin_src R :results output
  summary(as.factor(stream$symbol[]))
  #+end_src

  #+results:
  : +SK13   +SPH13  @EDM13  @EDU13  @ESH13  @ESM13  @F1M13  @JYM13  @N1M13  @NQM13
  :  299398     108  120731  167649  273192 2553308   27715  844995   27357 1285545
  : @T1M13  @USNM13 @VMJ13  @YMM13  CRDJ13  EBK13   EBM13
  :    1524   54804    3146 1216006  397696  917275  610827

- emini latency

  The latency pattern for the E-MINI SP500 (@ESM13 is the iqfeed code) is
  very similar to the overall latency pattern. The average latency over the
  day was:

  #+begin_src R
    require(ggplot2)
    require(bigvis)
    ind.emini = indexEQ(ind.symbol,"@ESM13 ")
    df1 = condense(bin(event.df$event.time[ind.emini],300,name="time"),bin(event.df$event.latency[ind.emini],0.05,name="latency"))
    df2 = df1[(df1$latency > 0) & (df1$latency < 2),]
    lat.av = tapply(df2$latency*df2$.count,df2$time,sum)/tapply(df2$.count,df2$time,sum)
    dfg = data.frame(Time=as.POSIXct(as.double(row.names(lat.av))+10*60*60,origin="1960-01-01", tz=""),Latency=lat.av)
    #colnames(dfg) = c("Time","Latency","Count")
    g = ggplot(data=dfg,aes(x=Time,y=Latency))
    g + geom_point()
    ggsave("quote-latency-averagecondensed.svg")
  #+end_src

  [[file:assets/quote-latency-averagecondensed.svg]]




* R database                                                              :R:

Experimenting with the mmap package in R, using this as a roll-your-own column database.

Starting with the raw market event stream:

** basic analytics
 
- Count Code Types
  
  #+begin_src R
    require("hash")
    #inFile = "data/stream.100k.txt" 
    inFile = "data/data.all.out.txt" 
    inCon = file(inFile, open = "r")  
    h <- hash()
      
    while (length(lines <- readLines(inCon, n=200, warn=FALSE)) > 0) {
      s = strsplit(lines,",")
      for (x1 in 1:length(s)) {
        c = s[[x1]][2]
        if (has.key(c,h)) {
          h[[c]] = h[[c]] + 1
        } else {
          h[[c]] = 1
        }
      }
    }
      
  #+end_src

  #+begin_src R :results output
  h
  #+end_src

  #+results:
=<hash> containing 6 key-value pair(s).
  F : 342
  P : 27269
  Q : 5645781
  S : 32
  T : 94324
  n : 5
=- split into code types
 #+begin_src R
   inFile = "data/data.all.out2.txt" 
   #inFile = "data/stream.100k.txt"
   outFiles = c("data/streamf.txt",
                "data/streamp.txt",
                "data/streamq.txt",
                "data/streams.txt",
                "data/streamt.txt",
                "data/streamo.txt")
   
   inCon = file(inFile, open="r")
   outCons = NULL
   outCons$f = file(outFiles[1], open="w")
   outCons$p = file(outFiles[2], open="w")
   outCons$q = file(outFiles[3], open="w")
   outCons$s = file(outFiles[4], open="w")
   outCons$t = file(outFiles[5], open="w")
   outCons$o = file(outFiles[6], open="w")
   ns=0
   while (length(lines <- readLines(inCon, n=200, warn=FALSE)) > 0) {
     s = strsplit(lines,",")
     for (x1 in 1:length(s)) {
       c = s[[x1]][2]
       if (c=="F") {
         writeLines(lines[x1],con=outCons$f)
       } else if (c== "P") {
         writeLines(lines[x1],con=outCons$p)
       } else if (c== "Q") {
         writeLines(lines[x1],con=outCons$q)
       } else if (c== "T") {
         writeLines(lines[x1],con=outCons$t)
       } else if (c== "S") {
         writeLines(lines[x1],con=outCons$s)
       } else {
         writeLines(lines[x1],con=outCons$o)
       } 
     }
   }
   
   close(outCons$f)
   close(outCons$p)
   close(outCons$q)
   close(outCons$s)
   close(outCons$t)
   close(outCons$o)
   
     
 #+end_src

 #+results:

** stream to mmap
*** makedb
:PROPERTIES:
:tangle:   R/makedb.R
:END:

#+begin_src sh :tangle no
#head -n 100 streamq.100k.txt > streamq.100.txt
cat header.txt streamq.100k.txt > streamqh.100k.txt
#+end_src


**** libraries
#+begin_src R
rm(list = ls())
require("mmap")
require("rindex")
require("plyr")
require(stringr)
#+end_src

#+results:
: TRUE


**** variables

#+begin_src R
# stream with field header
raw.stream = "streamqh"
# where the mmap db will be located
db.path = paste("data/",raw.stream,"/",sep="")
# mmap of the entire row
main.filename = paste("data/",raw.stream,"/main.data",sep="")
# file containing the raw feed
file.csv.data = paste("data/",raw.stream,".txt",sep="")
# maximum character length of the event field
event.size = 12
# maximum character length of the id field
id.size = 12

#+end_src

#+results:
: 12



**** slurp in raw data (mmap)

mmap.csv was difficult to work with when there were blanks entries. These
translated as NA when slurped up by read.table which is a logical type and
thus not supported by mmap.

Once past this hurdle, adhoc analysis of the larger data set is painless
despite size issues.


- mmap.csv hack
  #+begin_src R :results output
    
  my.mmap.csv = function(file,
    file.mmap = NA,
    header = TRUE, 
    sep = ",", 
    quote = "\"", 
    dec = ".", 
    fill = TRUE, 
    comment.char = "", 
    row.names,
    actualColClasses = NA,
    ...)
  {
      ncols <- length(gregexpr(sep, readLines(file, 1))[[1]]) + 
          1
      mcsv <- tempfile()
      tmplist <- vector("list", ncols)
      cnames <- character(ncols)
      if (!missing(row.names) && is.numeric(row.names) && length(row.names) == 
          1L) 
          ncols <- ncols - 1
      for (col in 1:ncols) {
          colclasses <- rep("NULL", ncols)
          if (!missing(actualColClasses)) {
            colclasses[col] <- actualColClasses[col]
          } else {
            colclasses[col] <- NA
          } 
          clm <- read.table(file = file, header = header, sep = sep, 
              quote = quote, dec = dec, fill = fill, comment.char = comment.char, 
              colClasses = colclasses, stringsAsFactors = FALSE, 
              row.names = row.names, ...)
          cnames[col] <- colnames(clm)
          tmplist[[col]] <- as.mmap(clm[, 1], force = TRUE)
      }
      stype <- do.call(struct, lapply(tmplist, function(X) X$storage.mode))
      totalsize <- sum(sapply(tmplist, nbytes))
      if (is.na(file.mmap)) {
        tmpstruct <- tempfile()
      } else {
        tmpstruct = file.mmap
      }
      writeBin(raw(totalsize), tmpstruct)
      tmpstruct <- mmap(tmpstruct, stype)
      for (col in 1:ncols) {
          tmpstruct[, col] <- tmplist[[col]][]
      }
      colnames(tmpstruct) <- cnames
      extractFUN(tmpstruct) <- as.data.frame
      tmpstruct
  }
  
    #+end_src

  #+results:

- store mmap'ed raw stream in m
    #+begin_src R
      
      dir.create(db.path)
      
      colclasses = as.vector(c("character", "character", "character", "numeric", "integer", "character",
        "numeric", "integer", "character", "integer", "numeric", "integer", "character",
        "numeric", "integer", "character", "character", "integer", "character", "character","character"))
      
      m = my.mmap.csv(file=file.csv.data, file.mmap=main.filename, header=TRUE, actualColClasses=colclasses)
      head(m)
      st = m$storage.mode
      ticker.length =  nbytes(st$Symbol) - 1
    #+end_src

    #+results:
    | character |
    | character |
    | character |
    | numeric   |
    | integer   |
    | character |
    | numeric   |
    | integer   |
    | character |
    | integer   |
    | numeric   |
    | integer   |
    | character |
    | numeric   |
    | integer   |
    | character |
    | character |
    | integer   |
    | character |
    | character |
    | character |


**** fields

#+begin_src R :tangle no
colnames(m[])
#+end_src

#+results:
| Stamp                    |
| Code                     |
| Symbol                   |
| Most.Recent.Trade        |
| Most.Recent.Trade.Size   |
| Most.Recent.Trade.TimeMS |
| Extended.Trade           |
| Extended.Trade.Size      |
| Extended.Trade.TimeMS    |
| Total.Volume             |
| Bid                      |
| Bid.Size                 |
| Bid.TimeMS               |
| Ask                      |
| Ask.Size                 |
| Ask.TimeMS               |
| Message.Contents         |
| TickID                   |
| Last.TimeMS              |
| Extra1                   |
| Extra2                   |



**** conversion to column db
:PROPERTIES:
:tangle:   R/makedb.R
:END:
***** create mmaps for each column
#+begin_src R
  stream = NULL
  stream$stamp = as.mmap(as.double(strptime(m[]$Stamp, "%H:%M:%OS",tz="GMT")),file=paste(db.path,"stamp.data",sep=""), mode=double())
  stream$code = as.mmap(as.character(m[]$Code),file=paste(db.path,"code.data",sep=""), mode=char(1))
  stream$symbol = as.mmap(as.character(m[]$Symbol),file=paste(db.path,"symbol.data",sep=""), mode=char(ticker.length))
  stream$trade = as.mmap(m[]$Most.Recent.Trade,file=paste(db.path,"trade.data",sep=""), mode=double())
  stream$vol = as.mmap(m[]$Most.Recent.Trade.Size,file=paste(db.path,"vol.data",sep=""), mode=integer())
  stream$tradetime = as.mmap(as.double(strptime(as.character(m[]$Most.Recent.Trade.TimeMS), "%H:%M:%OS",tz="GMT")),file=paste(db.path,"tradetime.data",sep=""), mode=double())
  stream$tradeex = as.mmap(m[]$Extended.Trade,file=paste(db.path,"tradeex.data",sep=""), mode=double())
  stream$volex = as.mmap(m[]$Extended.Trade.Size,file=paste(db.path,"volex.data",sep=""), mode=integer())
  stream$tradetimeex = as.mmap(as.double(strptime(as.character(m[]$Extended.Trade.TimeMS), "%H:%M:%OS",tz="GMT")),file=paste(db.path,"tradetimeex.data",sep=""), mode=double())
  stream$voltot = as.mmap(m[]$Total.Volume,file=paste(db.path,"voltot.data",sep=""), mode=integer())
  stream$bid = as.mmap(m[]$Bid,file=paste(db.path,"bid.data",sep=""), mode=double())
  stream$bidvol = as.mmap(m[]$Bid.Size,file=paste(db.path,"bidvol.data",sep=""), mode=integer())
  stream$bidtime = as.mmap(as.double(strptime(as.character(m[]$Bid.TimeMS), "%H:%M:%OS",tz="GMT")),file=paste(db.path,"bidtime.data",sep=""), mode=double())
  stream$ask = as.mmap(m[]$Ask,file=paste(db.path,"ask.data",sep=""), mode=double())
  stream$askvol = as.mmap(m[]$Ask.Size,file=paste(db.path,"askvol.data",sep=""), mode=integer())
  stream$asktime = as.mmap(as.double(strptime(as.character(m[]$Ask.TimeMS), "%H:%M:%OS",tz="GMT")),file=paste(db.path,"asktime.data",sep=""), mode=double())
  stream$event = as.mmap( str_pad(as.character(m[]$Message.Contents), event.size, side = "right", pad = " "),file=paste(db.path,"event.data",sep=""), mode=char(event.size))
  stream$id = as.mmap(m[]$TickID,file=paste(db.path,"id.data",sep=""), mode=integer())
  
#+end_src

#+results:

***** create indices using rindex

#+begin_src R
  require(rindex)
  ind.stamp = index(as.character(stream$stamp[]))
  ind.symbol = index(stream$symbol[])
  ind.event = index(stream$event[])
  ind.id = index(str_pad(as.character(stream$id[]), id.size, side = "left", pad = " "))
#+end_src

#+results:

***** save indexes

#+begin_src R
  
  fields = names(stream)
  save(list = c("ind.stamp",
         "ind.symbol",
         "ind.event",
         "ind.id",
         "fields",
         "main.filename",
         "st",
         "ticker.length",
         "event.size",
         "id.size"
         ),
       file = paste(db.path,".Rdbinfo",sep=""))
  
#+end_src

#+results:

*** reboot
:PROPERTIES:
:tangle:   R/reboot.R
:END:

#+begin_src R
  
  
  rm(list = ls())
  require("mmap")
  require("rindex")
  require("plyr")
  require("stringr")
  raw.stream = "streamqh"
  # where the mmap db is located
  db.path = paste("data/",raw.stream,"/",sep="")
  
  load(paste(db.path,".Rdbinfo",sep=""))
  #m = mmap(main.filename, mode=st)
  stream = NULL
  stream$stamp = mmap(paste(db.path,fields[1],".data",sep=""), mode=double())
  stream$code = mmap(paste(db.path,fields[2],".data",sep=""), mode=char(1))
  stream$symbol = mmap(paste(db.path,fields[3],".data",sep=""), mode=char(ticker.length))
  stream$trade = mmap(paste(db.path,fields[4],".data",sep=""), mode=double())
  stream$vol = mmap(paste(db.path,fields[5],".data",sep=""), mode=integer())
  stream$tradetime = mmap(paste(db.path,fields[6],".data",sep=""), mode=double())
  stream$tradeex = mmap(paste(db.path,fields[7],".data",sep=""), mode=double())
  stream$volex = mmap(paste(db.path,fields[8],".data",sep=""), mode=integer())
  stream$tradetimeex = mmap(paste(db.path,fields[9],".data",sep=""), mode=double())
  stream$voltot = mmap(paste(db.path,fields[10],".data",sep=""), mode=integer())
  stream$bid = mmap(paste(db.path,fields[11],".data",sep=""), mode=double())
  stream$bidvol = mmap(paste(db.path,fields[12],".data",sep=""), mode=integer())
  stream$bidtime = mmap(paste(db.path,fields[13],".data",sep=""), mode=double())
  stream$ask = mmap(paste(db.path,fields[14],".data",sep=""), mode=double())
  stream$askvol = mmap(paste(db.path,fields[15],".data",sep=""), mode=integer())
  stream$asktime = mmap(paste(db.path,fields[16],".data",sep=""), mode=double())
  stream$event = mmap(paste(db.path,fields[17],".data",sep=""), mode=char(12))
  stream$id = mmap(paste(db.path,fields[18],".data",sep=""), mode=integer())
  #save(list = ls(all=TRUE), file = paste(db.path,".Rsnap"))
  
#+end_src

#+results:


** stream -> price vector
*** event codes


| code | meaning                                             |
|------+-----------------------------------------------------|
|      |                                                     |
| E    | Extended Trade = Form T trade                       |
| O    | Other Trade = Any trade not accounted for by C or E |
| b    | A bid update occurred                               |
| a    | An ask update occurred                              |
| o    | An Open occurred                                    |
| h    | A High occurred                                     |
| l    | A Low occurred                                      |
| c    | A Close occurred                                    |
| s    | A Settlement occurred                               |

#+begin_src R :results output org
library(ascii)
options(asciiType="org")
ascii(summary(as.factor(stream$event[])),header=T,include.colnames=T)

#+end_src

#+results:
#+BEGIN_SRC org
|              | C            | Cba          | Cbal         | Ch           | Cl           | Cohl         | O            | a            | al           | b            | ba           | bh           |
|--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------|
| 6261.00      | 1246795.00   | 113.00       | 1.00         | 787.00       | 347.00       | 19.00        | 149310.00    | 3598298.00   | 5.00         | 3570299.00   | 228979.00    | 62.00        |
#+END_SRC


*** symbol event table

#+begin_src R :colnames yes :rownames yes 
  e = unique(as.factor(stream$event[]))
  s = unique(as.factor(stream$symbol[]))
  symbol.event.count = data.frame(array(NA,c(length(e),length(s))),row.names=e)
  colnames(symbol.event.count) = s
  
  for(x1 in 1:length(e)) {
    for(x2 in 1:length(s)) {
      symbol.event.count[x1,x2] = sum(stream$symbol[][stream$event[]==e[x1]]==s[x2])  
    }
  }
  
  symbol.event.count
#+end_src

#+results:
|      | @JYM13 | @YMM13 | EBK13 | EBM13 | @NQM13 | @EDM13 | @EDU13 | @ESM13 | +SK13 | @USNM13 | @F1M13 | @N1M13 | @T1M13 |
|------+--------+--------+-------+-------+--------+--------+--------+--------+-------+---------+--------+--------+--------|
| a    |   5120 |   8054 |  7122 |  4340 |   4962 |    930 |    568 |  11979 |   122 |     115 |     15 |     91 |      1 |
| b    |   5120 |   7505 |  6709 |  4689 |   4275 |    790 |    723 |  10144 |   310 |      85 |     47 |     74 |      1 |
| ba   |    288 |    595 |     0 |     0 |    307 |     58 |     75 |    437 |    23 |       0 |      1 |      3 |      1 |
| C    |   1374 |    817 |  2204 |   642 |   1032 |    482 |     94 |   4107 |    69 |       1 |      0 |      0 |      0 |
| O    |      0 |      0 |   518 |   697 |      0 |     55 |    179 |      0 |    50 |       0 |      0 |      0 |      0 |
| Cohl |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       1 |      0 |      0 |      0 |

*** one symbol processing of trades


#+begin_src R
  n = length(stream$event[])
  ts = indexEQ(ind.symbol,"@ESM13 ")
  tt = grep("C|O",stream$event[])
  tC = grep("C",stream$event[])
  tO = grep("O",stream$event[])
  
  trades = intersect(ts,tt)
  
  price = (stream$trade[][trades] * grepl("C",stream$event[][trades])) + 
           stream$tradeex[][trades] * grepl("O",stream$event[][trades])
  vol = (stream$vol[][trades] * grepl("C",stream$event[][trades])) + 
           stream$volex[][trades] * grepl("O",stream$event[][trades])
  id = stream$id[][trades]
  
  time = (stream$tradetime[][trades] * grepl("C",stream$event[][trades])) + 
           stream$tradetimeex[][trades] * grepl("O",stream$event[][trades])
  
  time.posix = as.POSIXct(time,origin="1960-01-01", tz="GMT")
  stamp = stream$stamp[][trades]
  
  voltot = stream$voltot[][trades]
  
  df = data.frame(price,vol,time.posix, voltot, stamp, id)
  summary(df)
#+end_src

#+results:
| Min.   :1549 | Min.   :  1.000 | Min.   :2003-04-10 14:00:01 | Min.   :      1 | Min.   :1.366e+09 | Min.   : 8205905 |
| 1st Qu.:1553 | 1st Qu.:  1.000 | 1st Qu.:2003-04-10 23:48:30 | 1st Qu.: 422931 | 1st Qu.:1.366e+09 | 1st Qu.: 9409562 |
| Median :1554 | Median :  2.000 | Median :2003-04-11 01:01:03 | Median : 793764 | Median :1.366e+09 | Median :10709956 |
| Mean   :1554 | Mean   :  5.052 | Mean   :2003-04-11 01:40:02 | Mean   : 794684 | Mean   :1.366e+09 | Mean   :10562990 |
| 3rd Qu.:1555 | 3rd Qu.:  4.000 | 3rd Qu.:2003-04-11 04:01:04 | 3rd Qu.:1159098 | 3rd Qu.:1.366e+09 | 3rd Qu.:11717004 |
| Max.   :1559 | Max.   :984.000 | Max.   :2003-04-11 13:59:58 | Max.   :1699828 | Max.   :1.366e+09 | Max.   :12749701 |


*** checks
- id sequence
- time sequence
- stamp sequence
- voltot = voltot + vol
#+begin_src R :results output
  sum(0 >= diff(df$id))
  sum(0 > diff(df$time))
  sum(0 >= diff(df$stamp))
  sum(df$vol[-1] != diff(df$voltot))
#+end_src

#+results:
: [1] 0
: [1] 0
: [1] 0
: [1] 0


*** voltot crosstable

#+begin_src R :colnames yes :rownames yes 
  e = unique(as.factor(stream$event[]))
  s = unique(as.factor(stream$symbol[]))
  symbol.event.stat = data.frame(array(NA,c(length(e),length(s))),row.names=e)
  colnames(symbol.event.stat) = s
  
  for(x1 in 1:length(e)) {
    for(x2 in 1:length(s)) {
      symbol.event.stat[x1,x2] = mean(stream$voltot[][stream$event[]==e[x1]]==s[x2])  
    }
  }
  
  symbol.event.stat
#+end_src

#+results:
|      | @JYM13 | @YMM13 | EBK13 | EBM13 | @NQM13 | @EDM13 | @EDU13 | @ESM13 | +SK13 | @USNM13 | @F1M13 | @N1M13 | @T1M13 |
|------+--------+--------+-------+-------+--------+--------+--------+--------+-------+---------+--------+--------+--------|
| a    |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |
| b    |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |
| ba   |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |
| C    |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |
| O    |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |
| Cohl |      0 |      0 |     0 |     0 |      0 |      0 |      0 |      0 |     0 |       0 |      0 |      0 |      0 |



*** price, vol and time plot

#+begin_src R
  require(ggplot2)
  g = ggplot(df, aes(x=time.posix,y=price,size=vol))
    g + geom_point()
#+end_src

#+results:

** reference
http://www.rinfinance.com/agenda/2012/talk/JeffRyan.pdf

[[http://useless-factor.blogspot.com.au/2011/05/why-not-mmap.html][Useless Factor: Why not mmap?]]

[[http://www.rinfinance.com/agenda/2012/talk/JeffRyan.pdf][‎www.rinfinance.com/agenda/2012/talk/JeffRyan.pdf]]

[[http://cran.r-project.org/web/packages/mmap/mmap.pdf][‎cran.r-project.org/web/packages/mmap/mmap.pdf]]

[[http://stackoverflow.com/questions/8005417/mmap-and-csv-files][r - mmap and csv files - Stack Overflow]]

[[https://github.com/hadley/lubridate/tree/master/R][lubridate/R at master · hadley/lubridate · GitHub]]

[[http://r.789695.n4.nabble.com/Update-price-data-on-disk-using-mmap-package-td4431101.html][Rmetrics - Update price data on disk using mmap package]]

[[http://en.wikipedia.org/wiki/Sigmoid_function][Sigmoid function - Wikipedia, the free encyclopedia]]

http://www-stat.stanford.edu/~tibs/ElemStatLearn/

http://www.bnosac.be/index.php/blog/26-massive-online-data-stream-mining-with-r





* algo

There's a few hypotheses guilding the evolution of the algorithm design:
- whatever the complexity of an algorithm, the outcome can usually be
  represented as a distributional forecast of a security price.
- specifically, this means that pairs (and other relative value) trades should be thought of as a
  (potentially) strong relationship between two securities rather than a
  forecast of relative value between two securities.  With HFT, it becomes
  problematic when there is an implicit assumption that two trades can occur
  simultaneously and with certainty.
- that a comparative advantage of the project is in algorithm design
  in addition to raw speed.

** Statistical Forecasting                                               :R:
CLOSED: [2013-04-17 Wed 16:23]
:LOGBOOK:
- State "DONE"       from ""           [2013-04-17 Wed 16:23]
:END:
:PROPERTIES:
:blog:     t
:top:      t
:END:

#+begin_html
<script type="text/javascript"
  src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
#+end_html

A bewildering, seemingly infinite number of approaches exist for statistical
forecasting of returns. An early mistake made in this choice can doom an
algorithmic approach to trading (whether high-frequency or low) from the very
beginning of the hunt.

My experience, however, is that many seemingly disparate techniques are, in
fact, the same basic calculation of simple statistics with varying parameters
and restrictions of the solution space.

*** the moving average

*Simple Moving Average (SMA)*

The workhorse of technical market analytics is the moving average. As an
example, the simplest and most famous technical signal is the 20-day moving
average of price.  When the price falls below the moving average sell, and
when it moves above the average, buy.  Reaching for my latex cheat sheet:

$signal = price_{0} - \frac{\sum\limits_{i=0}^{19}price_{i}}{20}$

where $price_{0}$ is current price (as at a close say) and $price_{19}$ the price 19 days ago.

Rearranging:

\begin{align}
signal = &\left( \begin{array}{c} 0.95 & -0.05 & ... & -0.05 \end{array} \right) \ast \\
&\left( \begin{array}{c} p_{0} & p_{1} & ... & p_{19} \end{array} \right)
\end{align}

So the signal can be expressed as a weighted sum of price where the weights add to zero.


Moving on to the next most popular moving average crossover, let's look at the
signal from that results from using the 100 day average crossing the 20 day
average (120 actually but I'm making the maths easy):

\begin{equation}
\begin{split}
signal &= &\frac{\sum\limits_{i=0}^{19}price_{-i}}{20} - \frac{\sum\limits_{i=0}^{99}price_{-i}}{100} &\\
&= &\left( \begin{array}{cccc} 0.04 & 0.04 & ... & -0.01 & ... & -0.01 \end{array} \right) \ast \\
&&\left( \begin{array}{c} p_{0} & p_{1} & ... & p_{20} & ... & p_{99} \end{array} \right)
\end{split}
\end{equation}


*Exponential Moving Average (EMA)*

An exponential moving average adopts a weighting scheme where the weight
geometrically decays at a constant rate of $(1 - \frac{1}{days})$ the further back in time
you go. Price crossing a 10-day EMA signal looks like:

\begin{equation}
\begin{split}
signal &= &price_{0} - \frac{\sum\limits_{i=0}^{\infty}0.9^{i} \ast price_{i}}{10} \\
&= &\left( \begin{array}{c} 0.9 & -0.09 & -0.081 & ... \end{array} \right) \ast \\
&&\left( \begin{array}{c} p_{0} & p_{1} & p_{2} & ...  \end{array} \right)
\end{split}
\end{equation}

So, no matter how many different moving average rules are combined, nor the alternative
ways of specifying a moving average, moving average signal rules all boil down to a weighted sum
calculation of price.

\begin{equation}
signal = \sum\limits_{i=0}^{n}(k_{i} \ast price_{i})
\end{equation}

where $k_{i}$ are weights adding to zero.

*return-based signals*

A signal based on historical prices can also be expressed in terms of
historical returns given:

\begin{align}
1 + return_{0} = \frac{price_{0}}{price_{1}}
\end{align}

and, more generally

\begin{equation}
\frac{price_{0}}{price_{n}} =
\end{equation}
\begin{equation}
\frac{price_{0}}{price_{1}} \ast \frac{price_{1}}{price_{2}} \ast \cdots \ast \frac{price_{(n-1)}}{price_{n}} =
\end{equation}
\begin{equation}
\prod\limits_{i=0}^{n-1}(1+return_{i}) \approxeq
\end{equation}
\begin{equation}
1 + return_{0} + return_{1} + \cdots + return_{n-1}
\end{equation}
(dropping the insignificant terms from the taylor series expansion)


Now being a generic signal, you can divide by anything you want and it's still
a signal.  Dividing by $price_{20}$ (I'm making the maths easier) gives:

\begin{align}
&(k_{0} \ast \frac{price_{0}}{price_{20}}) + (k_{1} \ast \frac{price_{1}}{price_{20}}) + \cdots + (k_{19} \ast \frac{price_{19}}{price_{20}}) \\
\Rightarrow &k_{0} \ast (1 + return_{0} + ... + return_{19}) + \\
&k_{1} \ast (1 + return_{1} + ... + return_{19}) + \\
&\cdots + \\
&k_{19} \ast (1 + return_{19}) \\
\Rightarrow&(\sum\limits_{0}^{19}k_{i}) + k_{0} \ast return_{0} + \\
&(k_{0} +k_{1}) \ast return_{1} + ... + \\
&\sum\limits_{0}^{19}k_{i} \ast return_{19}
\end{align}

The first term (sum of weights) is zero, so for our first price MA example
[SMA(20)] looks like this in return terms:

\begin{align}
signal = &0.95 \ast return_{0} +\\
&0.9 \ast return_{1} + ... +\\
&0.05 \ast return_{18}
\end{align}

So a price moving average signal is the same as a return moving average where
weights decline by 0.05 each time period (simple decay).

Before we leave technicals, it might be useful to show the EMA(10) example as
a code snippet (and with a few practical tweaks) compared to the SMA(20) from
the previous example:

*R code*
#+begin_src R :results graphics :file assets/rweights.png
  max.n = 20
  days = 10
  k.weights = -(1-(1/days))^(0:(max.n-1))/days # the ema weights
  k.weights[1] = 1 + k.weights[1] # the current price
  ema.weights = cumsum(k.weights)
  ema.weights = ema.weights/sum(ema.weights) # normalising so sum=1
  sma.weights = seq(0.95,0.00,by=-0.05)/sum(seq(0.95,0.00,by=-0.05))
  require(reshape)
  require(ggplot2)
  df.long = melt(data.frame(ema.weights,sma.weights,time=0:(max.n-1)),id="time")
  g = ggplot(data=df.long,
         aes(x=time, y=value,color=variable)) +
    geom_line()
  g
#+end_src

#+results:
[[file:assets/rweights.png]]

[[file:{lisp}(ob:blog-url BLOG){/lisp}/{lisp}(ob:blog-assets-dir BLOG){/lisp}/rweights.png]]

*** momentum signals

Momentum in standard finance literature is defined like this:

#+begin_quote
  When the return over the last 12 months is negative, go short the market, otherwise stay long
#+end_quote

Assuming there are 250 trading days in a year gives:

\begin{equation}
signal = \sum\limits_{i=0}^{249}return_{i}
\end{equation}
\begin{equation}
signal = 1 \ast return_{0} + 1 \ast return_{1} + ... + 1 \ast return_{249}
\end{equation}


The weights are different and the signal uses a longer series but the
underlying formulae structure is exactly the same.

*** volatility

The volatility of a return series is calculated as (20 day series say which
would be broadly comparable to the VIX):

\begin{align}
vol_{0} = &\sqrt(0.05 * (return_{0} - av)^{2} + 0.05 * (return_{1} - av)^{2} \\
&+ ... + 0.05 * (return_{19} - av)^{2})
\end{align}

Now one problem with statistical analysis of time series that standard fiaince
is all over is that this is often a bad estimate of underlying volatility, because
volatility is auto-conditional (future volatility is highly dependent on
recent historical volatility).

To deal with this, the workhorse of standard finance historical volatility
analysis is the GARCH model. This note is getting quite long so I wont bore
with the details but the guts of the model /uses an exponential weighting
scheme/ to calculate volatility in a manner spookily like the EMA
calculations in return space.

\begin{align}
vol_{0} = \sqrt(&0.1 * (return_{0} - av)^{2} + \\
&0.09 * (return_{1} - av)^{2} + ...)
\end{align}

Same as the standard volatility calculation but with a different weighting
scheme.

Now, one more trick and I'll get to the main point. The average return in the
volatility equation is there as an estimate of the underlying mean return.
Much of technical analysis and momentum research is actually alternative
specifications of a useful expected return approximation (once you constrain
the weights to add up to 1). We can thus insert our weighted return formulae
in the volatility formulae:

\begin{align}
E(return_{0}) = &0.1 * return_{0} + 0.09 * return_{1} + ... \\
E(vol_{0}) = \sqrt(&0.1 * (return_{0} - return estimate)^{2} + \\
&0.09 * (return_{1} - return estimate)^{2} + ...)
\end{align}
(the weighting schemes don't have to be the same)

*** weighted.historicals

The function below applies these calculations to an historical time series
where hist.weights$mean are the return weights and hist.weights$vol are the
volatility weights.

#+begin_src R :results silent
  weighted.historicals = function(rets, hist.weights) {
    hmean=as.double(filter(rets,hist.weights$mean,sides=1))
    hmean[1:length(hist.weights$mean)]=cumsum(rets[1:length(hist.weights$mean)]*t(as.matrix(hist.weights$mean)))
    xvol=(rets-hmean)^2
    hvol=as.double(filter(xvol,hist.weights$vol,sides=1))^0.5
    hvol[1:length(hist.weights$vol)]=cumsum(xvol[length(hist.weights$vol)]*t(as.matrix(hist.weights$vol)))^0.5
    weighted.historicals=data.frame(hmean=hmean, hvol=hvol)
  }
#+end_src

*** the point

This function encapsulates a very broad church of finance theory concerned
with forecasting future returns. Trend following and reversions, breakouts,
technical analysis, bollinger bands, momentum theory, stochastic volatility,
tweaks to option pricing, copulas (by extending the reasoning to correlations)
and many other concepts can all be represented as operations involving
calculating historical moments (average and volatility) using various
weighting schemes.

On the one hand, this increases the rigour in systemically forecasting
returns. In particular, accidentally combining two signals that you think are
different (orthogonal) to each other but are very similar in reality often
leads to spurious predictions.

On the other hand, it opens up a richer set of curves to test than the
standard set used in forecasting.

#+begin_src R :results silent
  mom.weights = rep(0.05,20)
  df.long = melt(data.frame(ema.weights,sma.weights,mom.weights,time=0:(max.n-1)),id="time")
  ggplot(data=df.long,
           aes(x=time, y=value,color=variable)) +
      geom_line()
#+end_src

[[file:{lisp}(ob:blog-url BLOG){/lisp}/{lisp}(ob:blog-assets-dir BLOG){/lisp}/mweights.png]]

#+html: <br>

A momentum signal doesn't have to be arrived at using a linear weighting
scheme - it makes much more sense if recent returns are more important
indicators of what will happen next compared with returns in the distant past.
Always using an exponential weighting scheme is like assuming that markets
behave exactly like atoms undergoing nuclear decay. There's no fundamental
reason why weights can't go negative and doing this would naturally
incorporate mean reversion potentials naturally with momentum forces.

Using this generalist approach to calculating historical statistics is a core
of the proposed system design.



** Signal Prediction                                                     :R:
CLOSED: [2013-04-22 Mon 09:17]
:LOGBOOK:
- State "DONE"       from ""           [2013-06-16 Sun 09:17]
- State "DONE"       from "TODO"       [2013-04-22 Mon 17:12]
:END:
:PROPERTIES:
:tangle:   R/signal.prediction.R
:blog:     t
:top:      t
:END:


#+begin_html
<script type="text/javascript"
  src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
#+end_html

The analytics below takes the weighted moment function (wmom[fn:1]) discussed in the
previous post and applies the thinking to daily data on the SP500. The main
purpose is to provide examples of:
- using wmom to replicate trading signals in a flexible and generic way.
- re-using wmom to estimate future return distribution and then apply that
  distribution to workshop future evolution of the signal.
- use of the bigvis package to compress and represent large data sets.

It's not commonly known that market signals involving moving averages are
forecastable. You know with certainty the returns that are dropping out of the
calculation and if we have some idea of the future stochastics, we can
estimate a distribution of return innovations that is anything but a random
walk. In other words, we can arrive at a probability that a signal will ping
without doing anything involving guessing market direction. This effect is
small, but as the analysis shows, there is a large and profitable gap between
perfect execution contemporaneous with a signal ping, and execution before
the ping based on future signal evolution that doesn't involve guessing future
market direction.

*** sp500 data

#+begin_src R :results silent
  rm(list = ls())
  require(xts)
  require(quantmod)
  options(warn=-1)
  Sys.setenv(TZ='GMT')
  getSymbols("^GSPC",src='yahoo', from='1927-01-01')
  sp500.price=na.omit(Cl(GSPC))
  sp500=log(sp500.price/lag(sp500.price,1))
  sp500=sp500[-1]
  colnames(sp500)="sp500"  
#+end_src

*** wmom

The function:
- first calculates the weighted mean for every day of the return series.
- fills in the initial mean values using the weights.  This is equivalent to
  assuming returns prior to the time series were zero.
- uses the mean series as a centering for the weighted volatility calculation.

#+begin_src R :results silent
  wmom = function(rets, weights) {
    mean=as.double(filter(rets,weights$mean,sides=1))
    mean[1:length(weights$mean)]=cumsum(rets[1:length(weights$mean)]*as.matrix(weights$mean))
    xvol=(rets-mean)^2
    vol=as.double(filter(xvol,weights$vol,sides=1))^0.5
    vol[1:length(weights$vol)]=cumsum(xvol[1:length(weights$vol)]*as.matrix(weights$vol))^0.5
    wmom=data.frame(mean=mean, vol=vol)
  }
#+end_src

- unit test
  #+begin_src R :results silent :tangle no
  rets=sp500
  weights=weights
  #wmom = function(rets, weights) {
    mean=as.double(filter(rets,weights$mean,sides=1))
    mean[1:length(weights$mean)]=cumsum(rets[1:length(weights$mean)]*as.matrix(weights$mean))
    xvol=(rets-mean)^2
    vol=as.double(filter(xvol,weights$vol,sides=1))^0.5
    vol[1:length(weights$vol)]=cumsum(xvol[1:length(weights$vol)]*as.matrix(weights$vol))^0.5
    wmom=data.frame(mean=mean, vol=vol)
  #}
  #+end_src

*** moments regression

Future volatility is highly forecastable, based on historical volatility and
historical mean return.  This always needs to be accounted for any statitical
analysis of time series and will often result in random varaites moving from
being fat-tailed to normal.

#+begin_src R :results output :exports both
  l=251
  weights=data.frame(mean=rep(1/l,l),vol=rep(1/l,l))
  m=wmom(sp500,weights)
  data=data.frame(sp500[-1],sp500[-1]^2,
    m$mean[-length(m$mean)],m$vol[-length(m$vol)]) #1 day delay in signal
  colnames(data)=c("sp500","sp500.vol","hmean","hvol")
  fit.mean = lm(sp500 ~ hmean + hvol,data=data)
  summary(fit.mean)
  fit.vol = lm(sp500.vol ~ hmean + hvol,data=data)
  summary(fit.vol)
#+end_src

#+results:
#+begin_example

Call:
lm(formula = sp500 ~ hmean + hvol, data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.229363 -0.004412  0.000192  0.004681  0.109353 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 6.146e-05  2.167e-04   0.284    0.777
hmean       1.026e-01  1.338e-01   0.767    0.443
hvol        2.198e-02  2.086e-02   1.053    0.292

Residual standard error: 0.009785 on 15924 degrees of freedom
Multiple R-squared:  7.934e-05,	Adjusted R-squared:  -4.625e-05 
F-statistic: 0.6317 on 2 and 15924 DF,  p-value: 0.5317

Call:
lm(formula = sp500.vol ~ hmean + hvol, data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.000542 -0.000076 -0.000039  0.000006  0.052346 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.993e-05  1.138e-05  -1.751     0.08 .  
hmean       -6.077e-02  7.029e-03  -8.645   <2e-16 ***
hvol         1.496e-02  1.096e-03  13.657   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0005139 on 15924 degrees of freedom
Multiple R-squared:  0.02527,	Adjusted R-squared:  0.02515 
F-statistic: 206.4 on 2 and 15924 DF,  p-value: < 2.2e-16
#+end_example

So the regression indicates that historical mean and volatility strongly
predicts return^2.

*** ma(20) price performance

A variation of wmom as we don't need a weighted volatility estimate for the
price ma(20) signal.

#+begin_src R
  wmean = function(rets, weights) {
    wm=as.double(filter(rets,weights,sides=1))
    wm[1:length(weights)]=cumsum(rets[1:length(weights)]*as.matrix(weights))
    wmean=wm
  }
#+end_src

#+results:

Weights as per the previous post analytics.

#+begin_src R :tangle no
  rets=sp500
  weights=seq(0.95,0.05,-0.05)
  #wmean = function(rets, weights) {
    wmean=as.double(filter(rets,weights,sides=1))
    wmean[1:length(weights)]=cumsum(rets[1:length(weights)]*as.matrix(weights))
  #}
#+end_src

And a move to analytics based on log(1+return) to remove compounding issues.

#+begin_src R
  sharpe = function(rets) {
    m=sum(log(1+rets))/length(rets)*251
    v=sd(log(1+rets))*(251^0.5)
    sharpe=m/v
  }
#+end_src

#+results:

The headline sharpe ratio for the signal is much higher than long-only.

#+begin_src R
  weights.ma20=seq(0.95,0.05,-0.05)
  m=wmean(sp500,weights.ma20)
  sig=as.double(m>0)
  sig=sig[-length(sig)]
  sharpe.long = sharpe(sp500[-1])
  sharpe.ma20 = sharpe(sp500[-1]*sig)
  c(sharpe.long,sharpe.ma20)
#+end_src

#+results:
| 0.380974902637794 |
| 0.606306306908276 |

| sharpe long-only    | 0.38 |
| sharpe price ma(20) | 0.61 |


The code below varies the delay between signal reading and trade execution.

#+begin_src R :results graphics :file assets/ma20.delay.png :exports both
  weights.ma20=seq(0.95,0.05,-0.05)
  m=wmean(sp500,weights.ma20)
  sig=as.double(m>0)
  sharpe.ma20.delay = rep(0,20)
  sharpe.long.delay = rep(0,20)
  
  for (x1 in 0:20) {
    sharpe.long.delay[x1+1] = sharpe(sp500[(x1+1):length(sp500)])
    sharpe.ma20.delay[x1+1] = sharpe(sp500[(x1+1):length(sp500)]*sig[1:(length(sp500)-x1)])
  }
  require(reshape)
  data = melt(data.frame(sharpe.long.delay, sharpe.ma20.delay, delay=0:20),id="delay")
  require(ggplot2)
    ggplot(data=data,
           aes(x=delay, y=value,color=variable)) +
      geom_line()
  
#+end_src

#+results:
[[file:assets/ma20.delay.png]]

The calculation of sharpe according to signal delay illustrates a few
important principles:
- a sharpe of 3.5 is what you get if you fail to be careful and introduce any
  sort of look-ahead bias. When humans look at price charts versus moving
  average lines, there is a failure to recognise that the apparently strong
  relationship is due to look-ahead bias.  The eye includes the price
  movement from before the lines cross which contains this large sharpe.
- signal effectiveness decays rapidly.  In this example, it is assumed that
  the calculation of the signal and the trade are assumed to occur
  simultaneously (at close).  Wait till next open and there is no excess
  sharpe.
- any forecasting ability, ahead of the actual ping, can be a significant
  profit opportunity and even in excess of the original signal sharpe.

*** turnover adjustment

#+begin_src R
  sharpe.costs = function(rets,turnover,cost) {
    m=sum(log(1+rets))/length(rets)*251-turnover*cost
    v=sd(log(1+rets))*(251^0.5)
    sharpe.costs=m/v
  }

#+end_src

#+results:

#+begin_src R :exports both
turnover = sum(abs(diff(sig)))/length(sig)*251
x1=1
scost = sharpe.costs(sp500[(x1+1):length(sp500)]*sig[1:(length(sp500)-x1)],turnover, 0.001)
#+end_src

#+results:
: 0.340845298070155

This signal is economically significant is trade costs are below 0.1%.


*** auto-conditionality estimates

Auto-conditionality is a made up term to represent the relationship between
historical mean and volatility of return and future distributional
characteristics.

parameter estimation

#+begin_src R :results output :exports both
  l=251
  weights=data.frame(mean=rep(1/l,l),vol=rep(1/l,l))
  m=wmom(sp500,weights)
  data=data.frame(sp500[-1],sp500[-1]^2,
    m$mean[-length(m$mean)],m$vol[-length(m$vol)]) #1 day delay in signal
  colnames(data)=c("sp500","sp500.vol","hmean","hvol")
  fit.mean = lm(sp500 ~ 0 + hmean + hvol,data=data)
  summary(fit.mean)
  fit.vol = lm((sp500.vol-0.005^2) ~ 0 + hmean + hvol,data=data)
  summary(fit.vol)
#+end_src

#+results:
#+begin_example

Call:
lm(formula = sp500 ~ 0 + hmean + hvol, data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.229373 -0.004402  0.000205  0.004693  0.109361 

Coefficients:
      Estimate Std. Error t value Pr(>|t|)    
hmean 0.121620   0.115859   1.050 0.293862    
hvol  0.027419   0.008171   3.356 0.000793 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.009785 on 15925 degrees of freedom
Multiple R-squared:  0.0009223,	Adjusted R-squared:  0.0007968 
F-statistic:  7.35 on 2 and 15925 DF,  p-value: 0.0006446

Call:
lm(formula = (sp500.vol - 0.005^2) ~ 0 + hmean + hvol, data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.000514 -0.000085 -0.000048 -0.000002  0.052353 

Coefficients:
        Estimate Std. Error t value Pr(>|t|)    
hmean -0.0746577  0.0060882  -12.26   <2e-16 ***
hvol   0.0109856  0.0004294   25.59   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0005142 on 15925 degrees of freedom
Multiple R-squared:  0.04205,	Adjusted R-squared:  0.04193 
F-statistic: 349.5 on 2 and 15925 DF,  p-value: < 2.2e-16
#+end_example

vol estimate
#+begin_src R :results output
  mean.est = 0.0003 + 0.12 * data$hmean + 0.011 * data$hvol
  vol.est = pmax(0.005^2 + -0.075 * data$hmean + 0.027 * data$hvol)^0.5
#+end_src

#+results:

*** signal forecasting

Armed with a concrete signal (1 (long) if price is above ma(20) price and 0
if price is below ma(20) price) and a distribution theory about return
stochastics (volatility is auto-conditional), a forecast of the distribution
signal can be performed so that a probability statement about the future
signal can be made.  

**** formulae

A bit of maths may help. The actual signal (the gap between current price and ma(20) of
price is a price signal which has been transformed into a return signal by
dividing by the oldest price ($price_{20}$). Tomorrows signal will be the same
return signal, but divided by $price_{19}$.

The difference between the signal today and the signal tomorrow is:

\begin{align}
signal_{today} &= 0.95 \ast p_{0} - 0.05 \ast p_{1} - ... -0.05 \ast p_{19}\\
signal_{tomorrow} &= 0.95 \ast p_{-1} - 0.05 \ast p_{0} - ... -0.05 \ast p_{18}\\
\delta{signal} &= 0.95 \ast p_{-1} - 1.0 \ast p_{0} + 0.05 p_{19}
\end{align}

In return terms, (dividing by current price: $price_{0}$)

\begin{multline}
\delta{signal}/price_{0} = 0.95 \ast return_{-1} - 0.05 \ast \sum_{0}^{18}return
\end{multline}

$return_{-1}$ is tomorrows return because the subscript convention was to
label returns backwards in time.


**** model

Modelling $r_{-1}$ as a random variate r ~ N(0,vol.est) enables the signal to
be turned into a probabilistic measure (what is the probability of
the signal being long tomorrow?).

#+begin_src R :results graphics :file assets/sig.example.png :exports both
price=c(0,cumsum(log(1+sp500)))
weights.av19 = rep(0.05,19)
weights.av20 = rep(0.05,20)
price.ma=as.double(filter(price,weights.av20,sides=1))
price.ma[1:length(weights.av20)]=cumsum(price[1:length(weights.av20)])/1:length(weights.av20)
gap=price-price.ma
st.mean.forecast = 0
return.forecast = st.mean.forecast-c(0,wmean(log(1+sp500),weights.av19))
price.ma.delta = c(0,price.ma[1:(length(price)-1)] * return.forecast[1:(length(price)-1)])
price.ma.forecast = c(0,price.ma[1:(length(price)-1)]) - price.ma.delta
gap.forecast.norm = (c(0,gap[1:(length(price)-1)])+price.ma.delta)/c(0.005,vol.est[1:(length(price)-1)])
sig.forecast = pnorm(gap.forecast.norm)
sig = as.double(gap>0)

df = data.frame(rets=c(0,sp500),price,price.ma,price.ma.forecast,sig,sig.forecast,
                 time=0:(length(price)-1))
colnames(df) = c("rets","price","price.ma","forecast.ma","sig","sig.forecast","time")

require(ggplot2)
require(reshape2)    

df.melt = melt(df[1:100,],measure.vars=c("price","price.ma","sig","sig.forecast"))

df.melt$type = c("sig","price")[as.double(is.element(df.melt$variable,as.factor(c("price","price.ma"))))+1]

ggplot(df.melt, aes(x = time, y = value)) +
  geom_line(aes(color = variable)) +
  facet_grid(type ~ ., scales = "free_y")
  #+end_src

#+results:
[[file:assets/sig.example.png]]

The example can be easily extended out beyond a day.

**** signal forecast and returns

Return histograms for various signal probabilities is shown below:

#+begin_src R :results graphics :file assets/sig.returns.png :exports both
require(bigvis)
require(grid)
tweak <- list(
  theme(
    legend.position = "bottom",
    plot.margin = unit(c(0, 0.5, 0, 0.5), "lines"),
    legend.key.width = unit(1, "inches"),
    text = element_text(size = 18)
  ),
  labs(x = NULL, y = NULL, fill = NULL),
  scale_fill_gradient(low="#e5e5e5", high = "#444548")
)

ret.sum = condense(bin(df$sig.forecast,0.02),bin(df$rets,0.002))
rs = peel(ret.sum,.95)
autoplot(rs) + tweak
#+end_src

#+results:
[[file:assets/sig.returns.png]]

**** mean and volatility variations

#+begin_src R :results graphics :file assets/sig.analysis.png :exports both
  require(bigvis)
  tweak <- list(
    scale_x_continuous("Signal Probabilistic Forecast", breaks = seq(0,1,0.2)),
    ylab(NULL),
    theme(
      plot.margin = unit(c(0, 0, 0, 0), "lines"),
      text = element_text(size = 18),
      panel.margin = unit(0.25, "cm")
    )
  )
  ret.sum = condense(bin(df$sig.forecast,0.02),z=df$rets, summary = "sd")
  
  d = data.frame(
    ret = rep(ret.sum$df.sig.forecast,3),
    summary = rep(c("count","mean","sd"), each = nrow(ret.sum)),
    value = c(ret.sum$.count, ret.sum$.mean*251, ret.sum$.sd*(251^.5))
  )
  
  smoothes <- list(
    count = smooth(ret.sum, 0.2, var = ".count", type = "robust"),
    mean = smooth(ret.sum, 0.2, var = ".mean", type = "robust"),
    sd = smooth(ret.sum, 0.2, var = ".sd", type = "robust"))
  smoothes$mean$.mean = smoothes$mean$.mean*251
  smoothes$sd$.sd = smoothes$sd$.sd*(251^0.5)
  
  smoothes <- Map(function(x, n) {
    names(x)[2] <- "value"
    x$summary <- n
    x
  }, smoothes, names(smoothes))
  smooth <- do.call(rbind, smoothes)
  smooth$summary <- factor(smooth$summary, levels = c("count", "mean", "sd"))
  levels(smooth$summary)[1] <- "count"
  
  qplot(ret, value, data = d, geom = "line") +
    facet_grid(summary ~ ., scales = "free") +
    tweak +
    geom_line(data = smooth, aes(x=df.sig.forecast, y=value),colour="#000099") 
    
  
#+end_src

#+results:
[[file:assets/sig.analysis.png]]

The chart above utilises the bigvis package to compress the return results
for the range of signal forecasts.

Clearly the price ma(20) signal is above all a proxy for volatility:
- a signal forecast of 0 (price clearly below the moving average of price) is
  strongly associated with increased volatility.  Volatility reduces sharpely
  for signal strength above zero and continues to decrease as signal
  increases.
- there is little difference between returns when the signal is clearly 1 and
  the returns when the signal is clearly 0. Although probably not
  statistically significant, there is some increase in the average return when
  the signal probabilities are around 0.5. This may be associated with
  resistance pressures on return when current price is at a popular moving
  average line.


**** Footnotes

[fn:1] refactored name.  Previous post referred to weighted.historical



** algo soup

A major part of this project is to invent a generic algorithm that
encapsulates most other algorithmic design ideas.

To start somewhere, the steps below represent a thought experiment on a
different approach to standard practices using pseudo-code.


*** step 1: boiler-plate trend following

The most common technical analysis in the world is to extract a trading signal
using the difference between a short-run and long-run moving average of price:

    n.short = 100000 n.long = 1000000 signal = sum(old.data.stream[1::n.long])
    / n.long - sum(old.data.stream[1::n.short]) / n.short


*** step 2: generalising this pattern

Both the long and the short ma are calculated on the old.data.stream and are
just different weighting schemes.

    weight = matrix(1/n.long, 1, n.long) - matrix (1/n.short,1,n.short)
    signal = sum(weight * old.data.stream[1::nlong])

*** step 3: formulating the weighting scheme

The weighting scheme:
- can be much more general than restricted to two simple moving averages
- can represent a large subset of technical trading rules
- artificially weights data that is one million ticks old the same as the last tick.

So,

    better.weighting.scheme = prior.smooth.curve.declining.to.zero
    optimise(better.weighting.scheme)
    signal = sum(better.weighting.scheme * old.data.stream[1::n.long])

*** step 4: convert to a vector method

convert the signal to a vector method involving old values of itself (this is
a tricky bit)

    signal[0] = some.function(tricky.weighting.scheme, signal[1:nlong],
    old.data.stream[1:n.long])

Remember that old signal values are also just weighted sums of
old.data.stream, so working out the tricky.weighting.scheme is pretty much
just tricky diff algebra.

It also means that some.function looks pretty similar to just a sum (but there
might be some non-linearity there)

*** step 5: reorient towards the event flow

Find that the importance of older old.data.stream values decays very quickly once you start to use past signal values.  So quickly, in fact, that old.data.stream[3] (say) has a fairly low weight even!  The importance of signal[1:n.long] decays less quickly.

So the signal calculation becomes:

    signal[0] = sum(tricky.weighting.scheme, signal[1:n.not.so.long], old.data.stream[1:3])

*** step 6: iterate

Iterate steps 3,4 & 5 on the *signal* (which is itself now an algerbaic iteration of a weighting scheme), each time coming up with a new weighting scheme that reduces the dependence on old data.

The new weighting scheme can also be thought of as a new signal.

    tricky.weights[0]=tricky.weighting.scheme
    signals[0] = signal
    for (x1=1:20)
        tricky.weights[x1] = optimise(minimise(n.not.so.long), signals[0:(x1-1)],tricky.weights[0:(x1-1)])
        signals[x1] = some.function(signals, tricky.weights)
    end for

Now step 6 is the very tricky bit I havent fully thought through.  I doubt a c loop will work for example.  And I'm probably trying to reinvent something that already exists, like a principle component method or something.  But what I think this process converges to is this:

    signals[0] = some.linear.function(signals[1], old.data.stream[0])
    => signals = funk(signals, linear.signal.functions, data.stream)

*** Finally ...

There are a few other narratives which support this:

The end result looks very neat from a mathematical and computational
point-of-view. This is the way the world is supposed to look, with an event
stream, an information state (the signals) and an algorithm that relates event
to change in state.

Having done all the tricky math, it's easy to relate the original algorithm to
the final result. In the original method: - there are 1,000,001 signals:
1,000,000 being the last one million prices and 1 being the calculated
signal. - there is 1 linear.function, the dual ma crossover. Or you could
think about it as 3 signals (long ma, short ma and the difference) in addition
to the old.data.

The whole exercise could be thought of a combined compression and optimisation
computation. Some parts of the story are simply about compressing the market
data so it can compactly sit on the event stream. The algorithm needs to be
transformed given it needs to operate on compressed data but it should spit
out about the same answer (or there will be a speed - accuracy tradeoff). But
some parts of the story are about looking at the algorithm in the light of
getting it on the stream and thinking about how to do it better.

*** the point

The general point to an algo soup is to insert the algorithms directly into
the event streamin the event stream as part of an actor
framework. In other words, algorithms in the stream
(functions/transforms/state-variables) shouldn't use stream history to recompute
themselves. They can only use the stream and other algorithms in the stream.

Consider the original algorithm (a simple moving average). Most algorithms
recalculate every time so they look at the last million or so ticks (data
events) every time they update themselves. Some might be smart enough to add
the latest data and drop off old values. But a true streaming algorithm doesnt
have to remember old values and can use an exponential decay method to keep
track of the moving average. Thus the moving average algorithm becomes a state
variable in the stream. And it also goes from being a summation of a million
values to being a few bit shifts ;)



*** pseudo summary

#+begin_example
better.weighting.scheme = prior.smooth.curve.declining.to.zero
optimise(better.weighting.scheme)
signal = sum(better.weighting.scheme * old.data.stream[1::n.long])
signal[0] = some.function(tricky.weighting.scheme, signal[1:nlong], old.data.stream[1:n.long])
signal[0] = sum(tricky.weighting.scheme, signal[1:n.not.so.long], old.data.stream[1:3])
tricky.weights[0]=tricky.weighting.scheme
signals[0] = signal
for (x1=1:20)
   tricky.weights[x1] = optimise(minimise(n.not.so.long), signals[0:(x1-1)],tricky.weights[0:(x1-1)])
   signals[x1] = some.function(signals, tricky.weights)
end for
signals[0] = some.linear.function(signals[1], old.data.stream[0])

=> signals = funk(signals, linear.signal.functions, data.stream)  
#+end_example




** weighted stats

*** statistical basics
:PROPERTIES:
:tangle:   R/algo.calcs.R
:END:
- [ ] introduce the notion of event time (modifying base statistics for trade
  volume, trade block size and time bursts)

**** moment calcs
:LOGBOOK:
CLOCK: [2013-04-11 Thu 15:46]--[2013-04-24 Wed 09:38] => 305:52
CLOCK: [2013-01-09 Wed 18:04]--[2013-01-13 Sun 13:10] => 91:06
:END:

picks up from the df created [[*one%20symbol%20processing%20of%20trades][here]]

#+begin_src R
  require(moments)
  source("R/functions.moments.R")
  num.quantiles=5
  length.ma=100
  wmean=t(rep(1,length.ma)/length.ma)
  wvol=wmean
  ret = diff(df$price)
  
  mhist=weighted.historicals(ret,list(mean=wmean,vol=wvol))
  
  qs.hmean=quantile(t(mhist$hmean), probs = (1/num.quantiles)*(1:num.quantiles))
  qs.hvol=quantile(t(mhist$hvol), probs = (1/num.quantiles)*(1:num.quantiles))
  qts.hmean=quantile.ts(mhist$hmean, qs.hmean)
  qts.hvol=quantile.ts(mhist$hvol, qs.hvol)
  
  dfg = cbind(times=df$time[-1],ret,mhist,qts.hmean,qts.hvol)
  
  g = ggplot(dfg,aes(x=times,y=hmean,color=hvol))
  g = g + geom_point(aes(group=1))
  g
  str(dfg)
#+end_src

#+results:

**** signal calc
#+begin_src R
  delay.signal=1
  sig=c(as.logical(rep(0,delay.signal)),
    dfg$hmean[1:(dim(dfg)[1]-delay.signal)])
  qs.sig=quantile(sig, seq(0.01,1,0.01))
  dfg$qts.sig=quantile.ts(sig,qs.sig)
  dfg$pos=as.numeric(dfg$qts.sig>27)
  summary(dfg$pos)  
#+end_src

#+results:



**** stats

#+begin_src R :colnames yes :rownames yes :exports results
  dfn = subset(dfg, select = -c(times) )  
  stats = return.to.stats.raw(dfn)
  rownames(stats) = colnames(dfn)
  stats
#+end_src

#+results:
|           |                  mean |                 std |                sharpe |              skewness |         kurtosis |
|-----------+-----------------------+---------------------+-----------------------+-----------------------+------------------|
| ret       | -4.45439854038269e-07 |  0.0639281360775152 | -6.96782170370425e-06 |  0.000553703374815122 | 15.4881656403179 |
| hmean     | -1.87084738696073e-07 | 0.00152524040456262 | -0.000122659180897926 |   0.00516980772248681 | 3.49203639731598 |
| hvol      |    0.0521547216459395 |  0.0364080818510718 |      1.43250396599524 |     0.161045558610194 | 2.30959331703425 |
| qts.hmean |     0.691436686070027 |    1.51250326594929 |     0.457147235074603 |      1.73032945999324 | 3.99404004012053 |
| qts.hvol  |      1.99952783375472 |     1.4146801981856 |       1.4134133186562 | -0.000113249234867658 | 1.69941490792223 |
| qts.sig   |      25.3681453488061 |    26.8559429781648 |     0.944600804724367 |      1.50434087431932 | 3.72252342870455 |
| pos       |     0.172859171517507 |   0.378125816487323 |     0.457147235074603 |      1.73032945999324 | 3.99404004012053 |

**** bucket stats

#+begin_src R :colnames yes :rownames yes :exports results
  b = bucket.stats(
    dfg$ret,
    dfg$qts.hmean, 
    dfg$qts.hvol,
    num.quantiles,
    delay.signal)
#+end_src

#+results:
|   | bcount.1 | bcount.2 | bcount.3 | bcount.4 | bcount.5 | br.1 |                br.2 |                br.3 |                br.4 |                br.5 |             babsr.1 |           babsr.2 |           babsr.3 |            babsr.4 |            babsr.5 |              bsqr.1 |              bsqr.2 |              bsqr.3 |              bsqr.4 |              bsqr.5 |             bstd.1 |             bstd.2 |             bstd.3 |             bstd.4 |             bstd.5 |
|---+----------+----------+----------+----------+----------+------+---------------------+---------------------+---------------------+---------------------+---------------------+-------------------+-------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------|
| 1 |   112498 |    90708 |    92240 |    82978 |    85801 |    0 | 0.00482041275300966 | 0.00329032957502168 | 0.00720672949456482 | 0.00944336313096584 | 0.00356450781347224 | 0.010823190898267 | 0.013014960971379 | 0.0192701679963364 | 0.0307251663733523 | 0.00089112695336806 | 0.00270993186929488 | 0.00325509540329575 | 0.00482055484586276 | 0.00769294646915537 | 0.0298518822640679 | 0.0518336291386203 |  0.056958794817013 | 0.0690555961889243 | 0.0872001031189334 |
| 2 |        0 |        0 |        0 |        0 |        0 |    0 |                   0 |                   0 |                   0 |                   0 |                   0 |                 0 |                 0 |                  0 |                  0 |                   0 |                   0 |                   0 |                   0 |                   0 |                  0 |                  0 |                  0 |                  0 |                  0 |
| 3 |        0 |        0 |        0 |        0 |        0 |    0 |                   0 |                   0 |                   0 |                   0 |                   0 |                 0 |                 0 |                  0 |                  0 |                   0 |                   0 |                   0 |                   0 |                   0 |                  0 |                  0 |                  0 |                  0 |                  0 |
| 4 |        0 |        0 |        0 |        0 |        0 |    0 |                   0 |                   0 |                   0 |                   0 |                   0 |                 0 |                 0 |                  0 |                  0 |                   0 |                   0 |                   0 |                   0 |                   0 |                  0 |                  0 |                  0 |                  0 |                  0 |
| 5 |        0 |    21296 |    20013 |    29263 |    26444 |    0 |  -0.020203324567994 | -0.0153275371008844 | -0.0205037077538188 | -0.0307063984268643 |                   0 | 0.020743332081142 | 0.017501124269225 | 0.0228445477223798 |  0.033353501739525 |                   0 | 0.00519170266716754 | 0.00438152700744516 | 0.00573249495950518 | 0.00836673725608834 |                  0 | 0.0691646801127115 | 0.0643956583879973 | 0.0728853515007623 | 0.0861634208992548 |


**** std chart

#+begin_src R :results silent
  require(reshape)
  require(scales)
  vol.cats=c("low vol", "low-mid vol", "mid vol", "high-mid vol", "high vol")
  ret.cats=c("low ret", "low-mid ret", "mid ret", "high-mid ret", "high ret")
  data = b$br
  which.name = "return"
  colnames(data)=vol.cats
  rownames(data)=ret.cats
  data.m = melt(data)
  colnames(data.m) = c("return.level","volatility.level", which.name)
  data.m$return.level = factor(data.m$return.level,levels=ret.cats)
  data.m$volatility.level = factor(data.m$volatility.level,levels=vol.cats)
  data.m <- transform(data.m, rescale = rescale(data.m[which.name]))
  p <- ggplot(data.m, aes(return.level, volatility.level)) + geom_tile(aes(fill = return), 
                                                    colour =   "white") 
  p + scale_fill_gradient(low = "white", high = "steelblue", space="Lab")
  p
  ggsave("pricevolheat.svg")
#+end_src



*** weighting function
:PROPERTIES:
:tangle:   R/functions.algo.R
:END:

takes a sum of exponential curves
maxn is total length of series (from time -1 to time -maxn)
slope is a parameter effecting steepness (0 is flat)
delay is delaying the curve by x time units

so slope=100, delay = 0 is yesterday
slope = 0, delay = 0 is a simple ma

#+begin_src R :results silent
  weighted.curve = function(maxn,slope,weight,delay) {
    c = rep(0,maxn)
    for (x1 in 1:length(slope)) {
      if (slope[x1] == 0) {
        c = c + weight[x1] * 
          c(rep(0,delay[x1]),
            rep(1/maxn,maxn-delay[x1]))
      } else {
        c = c + weight[x1] * 
          c(rep(0,delay[x1]),
            exp((-1:(delay[x1]-maxn))*slope[x1])*(exp(slope[x1])-1))
      }
    }
    weighted.curve = c / sum(c)
  }
#+end_src

#+begin_src R :tangle no
  require(ggplot2)
  maxn = 100
  slope = c(0.01,0,.1)
  weight = c(0.3,0.3,0.4)
  delay = c(0,10,0)
  t1 = weighted.curve(maxn,slope,weight,delay)
  df = data.frame(t1,time=1:100)
  g2 = ggplot(df,aes(y=t1,x=time))
  g2 = g2 + geom_point(aes(group=1))
  g2 = g2 + ylim(0,max(t1))
  g2
  sum(t1)
#+end_src

#+results:
: 1


*** old school

x is a vector representing slope, delay and weight

#+begin_src R
   require(nloptr)
   require(moments)
   source("R/functions.moments.R")
   delay.signal=1
   #ret = diff(log(trade.price[]),1)    
   test.funk = function(x,ret) {
     length.ma=as.integer(x[2])
     wmean = weighted.curve(length.ma,x[3:5],x[6:8],as.integer(x[9:11]))
     wvol= weighted.curve(length.ma,x[12:14],x[15:17],as.integer(x[18:20]))
     mhist=weighted.historicals(ret,list(mean=wmean,vol=wvol))  
     sig.mean=c(as.logical(rep(0,delay.signal)),
       mhist$hmean[1:(length(mhist$hmean)-delay.signal)])
     sig.vol=c(as.logical(rep(0,delay.signal)),
       mhist$hvol[1:(length(mhist$hmean)-delay.signal)])
     pos=sig.mean>x[21] & sig.vol<x[22]
     test.funk=-sum(ret*pos)
   }
   
  minx = c(1,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.01,0)
  initx = c(1,10,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,-0.01,0.01)
  maxx = c(1,100,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0.01,0.05)
  
  t1 = test.funk(initx,ret)
  funk=function (x) {-test.funk(x,ret)}
  
  local_opts <- list("algorithm" = "NLOPT_LN_BOBYQA",
                     "xtol_rel"  = 1.0e-5,
                     "max_time" = 5)
  opts <- list("algorithm" = "NLOPT_GN_MLSL",
               "xtol_rel"  = 1.0e-5,
               "maxeval"   = 1000000,
               "maxtime" = 60,
               "local_opts" = local_opts )
  
  
  res = nloptr(initx, 
         funk, 
         eval_grad_f = NULL,
         lb = minx, 
         ub = maxx, 
         eval_g_ineq = NULL, 
         eval_jac_g_ineq = NULL,
         eval_g_eq = NULL, 
         eval_jac_g_eq = NULL,
         opts = opts
         )
  
  sol = res$solution
#+end_src

#+results:
|                    1 |
|     81.3259063172952 |
|                    1 |
|    0.468192098049262 |
|                    1 |
|     0.99355170757133 |
|                    0 |
|    0.839869048670139 |
|                    0 |
|                    0 |
|                    0 |
|                    0 |
|                    0 |
|                    0 |
|                    1 |
|    0.972759354860443 |
|                    0 |
|                    0 |
|                    0 |
|                    0 |
| -0.00907130114400014 |
|                 0.05 |


#+begin_src R
funk(sol)
#+end_src

#+results:
: -1917.25


*** event school

event school simulates pushing an algorithm through the event stream, so there is no look back.
The equivalent to a weighting scheme is n stats of the form:

x[n][t] = k[1] * x[i] + decay * x[n][t-1]
x[0][t] = r
x[1][t] = r^2
signal = all(x[i]<s)


So we have the first two nodes as the last return and squared return. We then
have a calculation using those two values that is similar to an exponential
moving average on the mean and garch on the volatility (or both). Then we have
another exactly the same, but able to also use the value of the third node. So
lets say we have a theory that we should go long the market after it has
crashed but the volatility has settled down: dropping time subscripts...

node[3] = 0.9 * node[3] + 0.09 * node[1] (ema of the mean return) 
node[4] = 0.9 * node [4] + 0.09 * node [2] (garch representation of short run
  volatility) 
node[5] = 0.99 * node [5] + 0.02 * node[2] (garch of the longer
  run volatility) 
signal = node[3] < large negative and node[4] < mid-level vol
  and node[5] > long-run mid-level vol


#+begin_src R
  require(nloptr)
  require(moments)
  source("R/functions.moments.R")
  delay.signal=1
  ret = diff(log(trade.price[]),1)    
  
  test.event.funk = function(x) {
    r=0
    n=as.integer(x[1])
    s=rep(0,n+2)
    for (x1 in 1:length(ret)) {
      s[1]=ret[x1]
      s[2]=ret[x1]^2
      for (x2 in 1:n) {
        s[x2+2] = x[(x2-1)*(n+2)+2] * s[x2+2] 
        for (x3 in 1:(x2+1)) {
          s[x2+2] = s[x2+2] + x[(x2-1)*(n+2)+x3+2] * s[x3]
        }
      }
      sig = s[n+2] < x[n*n+1] 
      r = r + ret[x1]*sig    
    }
    test.event.funk = -r
  }
  
  test.event.signals = function(x) {
    n=as.integer(x[1])
    s=rep(0,n+2)
    for (x1 in 1:length(ret)) {
      s[1]=ret[x1]
      s[2]=ret[x1]^2
      for (x2 in 1:n) {
        s[x2+2] = x[(x2-1)*(n+2)+2] * s[x2+2] 
        for (x3 in 1:(x2+1)) {
          s[x2+2] = s[x2+2] + x[(x2-1)*(n+2)+x3+2] * s[x3]
        }
      }
    }
    test.event.signals=s
  }
  
  
  
  minx = c(3,
    .4,0,0,0,0,
    .4,0,0,0,0,
    .4,0,0,0,0,
    0)
  
  initx = c(3,
    .9,.1,0,0,0,
    .91,0,.11,0,0,
    .92,0,0,.12,.13,
    0.0001)
  
  maxx = c(3,
    .999,.1,.1,0,0,
    .999,.1,.1,.1,0,
    .999,.1,.1,.1,.1,
    0.01)
  
  
  t1 = test.event.funk(initx)
  
  local_opts <- list("algorithm" = "NLOPT_LN_BOBYQA",
                     "xtol_rel"  = 1.0e-5,
                     "max_time" = 5)
  opts <- list("algorithm" = "NLOPT_GN_MLSL",
               "xtol_rel"  = 1.0e-5,
               "maxeval"   = 1000000,
               "maxtime" = 60,
               "local_opts" = local_opts )
  
  
  res = nloptr(initx, 
         test.event.funk, 
         eval_grad_f = NULL,
         lb = minx, 
         ub = maxx, 
         eval_g_ineq = NULL, 
         eval_jac_g_ineq = NULL,
         eval_g_eq = NULL, 
         eval_jac_g_eq = NULL,
         opts = opts
         )
  
  sol = res$solution
#+end_src

#+results:
|                    1 |
|     96.7682507574266 |
|     0.84432180324621 |
|    0.105146308188681 |
|    0.954422686999946 |
|    0.969834907324997 |
|    0.679679146533096 |
|    0.446552514630711 |
|                    0 |
|                    0 |
|                    0 |
|    0.589999080305931 |
|     0.32101020573405 |
|    0.635467572036928 |
|    0.902063345491363 |
|    0.524241630483691 |
|    0.814966178608965 |
|                    0 |
|                    0 |
|                    0 |
| -0.00836017778367073 |
| 0.000451276085862017 |


*** TODO complex event space

The above results are meaningless due to not taking into account a variety of
extraneous variables:
- market open and closing times
- volume
- trade size

To correct this, the dimension over which volatility and other statistics are
calculated should be abstracted.  Passage of calendar time may be one event (1 min, 30
sec etc), volume bucketing another (1 lot, 5 lots, 100 lots...), trade size is
another (1 lot, 5 lots, 100 lots). 

For example, say a single event is defined as volume*1+trades*1.5 > 2n.
Price is recorded whenever this 'event' occurs and statistical calculations
proceed as per the normal case.

As long as everything is linear the optimization should stay convex.

*** distributional approach

Let's say we have an estimate of future returns based on historical returns as follows:

return t+1 = 0.1 * historical (conditional) mean + 0.9 unconditional mean
volatility t+1 = 0.8 * historical (conditional) vol + 0.2 unconditional vol

You can have many different historical mean and vol measurements included in
the above. Conditional means and vols can also be created from exogenous
variables (stat arb)

We then have a distribution forecast for t+1.  So:

E(actual return t+1 - forecast return)/forecast std) being normally
distributed is an interesting test.

Back solving for which historical means and vols (with weighting schemes as
the free variable) that cause error terms to be normally distributed.

- calc conditionals
- calc forecast

#+begin_src R
  source("R/functions.algo.R")
  source("R/functions.moments.R")
  length.ma = 100    
  wmean = weighted.curve(length.ma,c(0.1,0,0),c(1,0,0),c(0,0,0))
  wvol= weighted.curve(length.ma,c(0.1,0,0),c(1,0,0),c(0,0,0))
  mhist=weighted.historicals(ret,list(mean=wmean,vol=wvol))  
  w = c(.2,.8,.9,.1)
  un = c(0,.00001)  
  forecast.mean = rep(0,length(ret))
  forecast.mean[2:length(ret)] = w[1] * mhist$hmean[1:length(ret)-1] + w[2] * un[1]
  forecast.std = rep(0,length(ret))
  forecast.std[2:length(ret)] = w[3] * mhist$hvol[1:length(ret)-1] + w[4] * un[2]
  forecast.error = (ret - forecast.mean)/forecast.std
  qplot(forecast.mean)
#+end_src

#+results:

**** forecast examples

***** one point forecast
#+begin_src R
  source("R/functions.algo.R")
  source("R/functions.moments.R")
  length.ma = 100    
  wmean = weighted.curve(length.ma,c(0.1,0,0),c(1,0,0),c(0,0,0))
  wvol= weighted.curve(length.ma,c(0.1,0,0),c(1,0,0),c(0,0,0))
  mhist=weighted.historicals(ret,list(mean=wmean,vol=wvol))  
  w = c(.2,.8,.9,.1)
  un = c(0,.001)  
  forecast.mean = rep(0,length(ret))
  forecast.mean[2:length(ret)] = w[1] * mhist$hmean[1:length(ret)-1] + w[2] * un[1]
  forecast.std = rep(0,length(ret))
  forecast.std[2:length(ret)] = w[3] * mhist$hvol[1:length(ret)-1] + w[4] * un[2]
  forecast.error = (ret[-1] - forecast.mean[-1])/forecast.std[-1]
  qplot(forecast.mean)
  summary(forecast.error)  
#+end_src

#+results:



***** TODO n point forecast at t
instead of ret being used we could:
- do a simulation
- calc the one sd and mean bands


*** moment functions
:PROPERTIES:
:tangle:   R/functions.moments.R
:END:

***** startup
#+begin_src R
  momentum.startup = function() {
    rm(list = ls())
    p=NULL
    require(xts)
    require(moments)
    require(nloptr)
    require(quantmod)
    require(ascii)
    require(plyr)
    require(ggplot2)
    options(warn=-1)
  }
#+end_src

***** return.to.stats.raw

#+begin_src R :results silent
  return.to.stats.raw = function(r) {
    r=na.omit(r)
    mean=apply(r,2,sum)/dim(r)[1]
    std=apply(r,2,sd)
    sharpe=mean/std
    skewness=apply(r,2,skewness)
    kurtosis=apply(r,2,kurtosis)
    
    return.to.stats.raw = data.frame(
      mean=mean,
      std=std,
      sharpe=sharpe,
      skewness=skewness,
      kurtosis=kurtosis)
  }
#+end_src

***** return.to.stats
#+begin_src R :results silent
  return.to.stats = function(xts.r) {
    r=na.omit(xts.r)
    n=dim(r)[1]
    years=as.numeric(difftime(index(r[n]),index(r[1]), units="days")/365.25)
    days.per.year=n/years
    mean=apply(r,2,sum)/years
    std=apply(r,2,sd)*sqrt(days.per.year)
    sharpe=mean/std
    skewness=apply(r,2,skewness)
    kurtosis=apply(r,2,kurtosis)
      
    return.to.stats = data.frame(
      mean=mean,
      std=std,
      sharpe=sharpe,
      skewness=skewness,
      kurtosis=kurtosis)
  }
#+end_src

***** weighted.historicals
#+begin_src R :results silent
  weighted.historicals = function(rets, hist.weights) {
    hmean=as.double(filter(rets,hist.weights$mean,sides=1))
    hmean[1:length(hist.weights$mean)]=cumsum(rets[1:length(hist.weights$mean)]*t(as.matrix(hist.weights$mean)))
    xvol=(rets-hmean)^2
    hvol=as.double(filter(xvol,hist.weights$vol,sides=1))^0.5
    hvol[1:length(hist.weights$vol)]=cumsum(xvol[length(hist.weights$vol)]*t(as.matrix(hist.weights$vol)))^0.5
    weighted.historicals=data.frame(hmean=hmean, hvol=hvol)
  }
#+end_src

- unit test
  #+begin_src R :tangle no
    hist.weights = list(mean=wmean,vol=wvol)
    rets2 = coredata(rets)  
    hmean=as.double(filter(rets2,hist.weights$mean,sides=1))
    hmean[1:length(hist.weights$mean)]=cumsum(rets2[1:length(hist.weights$mean)]*t(as.matrix(hist.weights$mean)))
    xvol=(rets2-hmean)^2
    hvol=as.double(filter(xvol,hist.weights$vol,sides=1)^0.5)
    hvol[1:length(hist.weights$vol)]=cumsum(xvol[length(hist.weights$vol),]*t(as.matrix(hist.weights$vol)))^0.5
    summary(data.frame(hmean=hmean, hvol=hvol))
  #+end_src

  #+results:
  | Min.   :-2.588e-03 | Min.   :0.0004901 |
  | 1st Qu.:-6.261e-05 | 1st Qu.:0.0062086 |
  | Median : 3.594e-04 | Median :0.0077499 |
  | Mean   : 2.792e-04 | Mean   :0.0088680 |
  | 3rd Qu.: 7.081e-04 | 3rd Qu.:0.0103454 |
  | Max.   : 2.074e-03 | Max.   :0.0286538 |

***** quantile.ts
#+begin_src R :results silent
quantile.ts = function(ts,q) {
  n=length(ts)
  nq=length(q)
  quantile.ts=
  apply(matrix(ts,n,nq)>t(matrix(q,nq,n)),1,sum)
}
#+end_src

***** bucket.stats.posandr
#+begin_src R :results silent
  bucket.stats.posandr = function(pos, r, qts1, qts2, num.quantiles, delay.signal) {
    br=matrix(0,num.quantiles,num.quantiles);bcount=br;bpos=br;c=br;babsr=br;bsqr=br;bstd=br;
    for (i in 1:num.quantiles){
      for (j in 1:num.quantiles){
        sb=(qts1==(i-1))&(qts2==(j-1))
        sb=c(as.logical(rep(0,delay.signal)), sb[-(length(sb):(length(sb)-delay.signal))])
        bcount[i,j]=sum(sb)
        if (bcount[i,j]>0){
          br[i,j]=mean(r[sb])
          bpos[i,j]=mean(pos[sb])
          babsr[i,j]=mean(abs(r[sb]))
          bsqr[i,j]=mean(r[sb]^2)
          bstd[i,j]=apply(r[sb],2,sd)
        }
      }
    }
    bucket.stats=list(
      bcount=bcount,
      br=br,
      bpos=bpos,
      babsr=babsr,
      bsqr=bsqr,
      bstd=bstd)
  }
#+end_src

***** bucket.stats
#+begin_src R :results silent
  bucket.stats = function(r, qts1, qts2, num.quantiles, delay.signal) {
    br=matrix(0,num.quantiles,num.quantiles);bcount=br;c=br;babsr=br;bsqr=br;bstd=br;
    for (i in 1:num.quantiles){
      for (j in 1:num.quantiles){
        sb=(qts1==(i-1))&(qts2==(j-1))
        sb=c(as.logical(rep(0,delay.signal)), sb[-(length(sb):(length(sb)-delay.signal))])
        bcount[i,j]=sum(sb)
        if (bcount[i,j]>0){
          br[i,j]=mean(r[sb])
          babsr[i,j]=mean(abs(r[sb]))
          bsqr[i,j]=mean(r[sb]^2)
          bstd[i,j]=sd(r[sb])
        }
      }
    }
    bucket.stats=list(
      bcount=bcount,
      br=br,
      babsr=babsr,
      bsqr=bsqr,
      bstd=bstd)
  }
#+end_src

- unit test
  #+begin_src R :tangle no
    qts1 = res$qts.ret
    qts2 = res$qts.vol
    r = res$rets
    br=matrix(0,num.quantiles,num.quantiles);bcount=br;c=br;babsr=br;bsqr=br;bstd=br;
    for (i in 1:num.quantiles){
      for (j in 1:num.quantiles){
        sb=(qts1==(i-1))&&(qts2==(j-1))
        sb=c(as.logical(rep(0,delay.signal)), sb[-(length(sb):(length(sb)-delay.signal))])
        bcount[i,j]=sum(sb)
        if (bcount[i,j]>0){
          br[i,j]=mean(r[sb])
          babsr[i,j]=mean(abs(r[sb]))
          bsqr[i,j]=mean(r[sb]^2)
          bstd[i,j]=apply(r[sb],2,sd)
        }
      }
    }
    data.frame(
      bcount=bcount,
      br=br,
      babsr=babsr,
      bsqr=bsqr,
      bstd=bstd)
#+end_src

  #+results:
  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |

***** print.qstat

#+begin_src R
print.qstat = function(stat,count) {
  row.count = apply(count,1,sum)
  col.count = apply(count,2,sum)
  all.count = sum(row.count)
  row.stat = apply(stat*count,1,sum)/row.count
  col.stat = apply(stat*count,2,sum)/col.count
  all.stat = sum(row.stat*row.count)/all.count
  out=rbind(cbind(stat,row.stat), c(col.stat,all.stat))
  colnames(out)=c("low", "low-mid", "mid", "high-mid", "high", "all")
  rownames(out)=c("low", "low-mid", "mid", "high-mid", "high", "all")
  print.qstat=out }
#+end_src

***** print.qstat.count

#+begin_src R
print.qstat.count = function(count) {
  row.stat = apply(count,1,sum)
  col.stat = apply(count,2,sum)
  all.stat = sum(row.stat)
  out=rbind(cbind(count,row.stat), c(col.stat,all.stat))
  colnames(out)=c("low", "low-mid", "mid", "high-mid", "high", "all")
  rownames(out)=c("low", "low-mid", "mid", "high-mid", "high", "all")
  print.qstat=out }
#+end_src









** algorithm classifications

This is a summary of algorithm classification that will eventually become coding specifications.

The lists below are examples of categories and not meant to be exhaustive.

*** data-set choice
**** Price Endogenous

***** Algorithms that are endogenous to price

- moment-based calculations
  - moving average
  - GARCH
  - volatility
  - momentum
- technical analytics

***** Market Structure Endogenous

Algorithms that are endogenous to Price and market components closely related to a single security such as:
- volume
- bid/ask
- market depth, order book

***** Statistical Arbitrage

Algorithms that look to exploit near-arbitrage bound relationships between securities. 
- VIX versus SP500 versus options
- cross-correlation or lead-lag relationships between securities


**** Exogenous

Algorthims that seek to exploit relationships between price and factors external to the security market price information set.

- fundamentals
- twitter mentions
- earnings announcement/ event-based analytics

**** algorithmic theoretica

Ways to perform forecasting

- linear 
  - regression
- non-linear
  - NN
  - GA
- parameter fit 
  - local versus global minima
  - stationary versus non-stationary (versus semi-stationary)

**** risk management

***** position
- leverage employed
- P&L distribution choices (objective function)

***** trading
- transactional cost importance
- entry and exit methodologies
- security selection




* references
** messaging

http://msgpack.org/

[[http://code.google.com/p/protobuf-net/][Protocol Buffers]] 
[[https://code.google.com/p/quickfast/][FAST]] 
[[http://activequant.org/svn/aq2o/trunk/base/src/main/proto/messages.proto][ActiveQuant Google Protocol Buffer example]]

http://programmers.stackexchange.com/questions/121592/what-to-look-for-in-selecting-a-language-for-algorithmic-high-frequency-trading

http://stackoverflow.com/questions/731233/activemq-or-rabbitmq-or-zeromq-or

http://wiki.msgpack.org/pages/viewpage.action?pageId=1081387

http://kenai.com/downloads/javafx-sam/EventProcessinginAction.pdf

http://coffeeonesugar.wordpress.com/2009/07/21/getting-started-with-esper-in-5-
minutes/


** links to HN discussions

https://news.ycombinator.com/item?id=5550930

http://prof7bit.github.io/goxtool/

https://github.com/fmstephe/matching_engine

https://github.com/brotchie/ib-zmq

https://github.com/brotchie/r-zerotws

http://hackage.haskell.org/package/HLearn-algebra

http://bitcoin.clarkmoody.com/

** disruptor

The disruptor scheme is at the center of many peoples thinking about
low-latency and process design for HFT, and the benchmark for cutting-edge low-latency.

The key concepts (and this is a lame list to get started - please read the
references) include:

- the disruptor is a queue designed with the idea that queues are most often
  empty or full. The queue is a cyclical array (called a ring buffer) with a
  buffer size equal to a power of 2.
- (often) having a single writer (called producer) to avoid cache line contention
- multiple readers (consumers) that have dependency relationships with each
  other with these dependencies known to the queue.
- checks (memory barriers) simply guarding queue over- and under- runs (producers
  getting too far ahead or consumers being too fast)
- polling information (on producers and consumers) via the queue.  Recent
  post og the groups suggest consumer polling might be better. 
- achieving lock-free concurrency via producer separation and consumer
  tracking (called the consumer cursor)
- avoids (L2?) cache misses with variable padding
- in recent times, the 'worker pool' is leading to large performance
  gains (I don't know what it is exactly - just repeating the group
  discussions)

I'd be interested in feedback on porting these concepts to haskell:
- I don't see anything in hackage remotely similar, but I do see plenty of
  comparable concepts like cache-miss avoidance, STM worker pools etc etc. 
- this would provide quite a good reference for how much slower haskell is
  compared with imperative-style methods.
- (for me), the translation of the concepts to haskell is difficult.  So much
  of the disruptor design pattern is imperative (do this exactly given system
  architecture). Thinking in terms of promises and 'what is' may lead to a
  very different look and feel, and good insight as to the differences
  haskell brings to the problem domain.
- I suspect that haskell could start to perform very well when there are
  multiple writers (producers) and when there are a large number of
  interdependent readers (consumers). I have zero proof of this - just a gut
  feel.
- if there is a need to get closer to the metal than haskell can offer, it's
  best to find this out now and understand why. In this case I imagine the
  disruptor as a language-independent messaging solution superior to Protocol Buffers.


A few references to help:

Active info/code hubs

Main project
[[https://github.com/LMAX-Exchange/disruptor]]
http://lmax-exchange.github.io/disruptor/

Collections
https://github.com/LMAX-Exchange/LMAXCollections

Interesting OSS using disruptor
http://storm-project.net/

Clojure Library and DSL
http://userevents.github.io/phaser/

Blogs
http://blogs.lmax.com/

Martin Thompson blog
http://mechanical-sympathy.blogspot.co.uk/

Michael Barker blog
http://bad-concurrency.blogspot.com.au


Less active/reference

https://github.com/fsaintjacques/disruptor--
Performance in C++ is way down compared to main java repo.

Main technical Doc (2 years old now)
https://github.com/LMAX-Exchange/disruptor/tree/master/docs

trisha's posts

http://mechanitis.blogspot.com.au/2011/06/dissecting-disruptor-whats-so-special.html

Original Martin Fowler piece
[[http://martinfowler.com/articles/lmax.html][Martin Fowler]]

http://www.aurorasolutions.org/over-6-million-transactions-per-second-in-a-real-time-system-an-out-of-the-box-approach/


* gh-pages production

#+STYLE_DIR: ../style
#+ASSETS_DIR: assets
#+TEMPLATE_DIR: ~/stuff/site/templates
#+PUBLISH_DIR: ~/git/hyperq.github.io
#+URL: http://hyperq.github.io
#+DEFAULT_CATEGORY: blog
#+ANALYTICS: UA-22236293-1
#+DISQUS: scarce
#+POSTS_FILTER: +blog="t"
#+LaTeX_CLASS: scarce-org-article
#+FEEDS: hft
#+PROPERTY: category blog

** blogging instructions
Anyone who would like to publish bits and bobs is welcome to do so, and
everyone is encouraged to as well.

The project site ([[http:hyperq.github.io][hyperq.github.io]]) is
- a static site located in https://github.com/hyperq/hyperq.github.io
- published using the material in hyperq.org (this file)
- published using [[https://github.com/renard/o-blog][o-blog]]

So to publish a blog, you just:
- alter this file, typically by adding a new subtree below
- add a blog: t properties (like the other blog posts - follow your nose)
- throw images and charts in the assets directory
- send a pull request

Once the pull is merged, the blog will appear the next time I rebuild the
site (which I have down to 2 button pushes if there are no snaffoos, which
there usually is)

If anyone would like more info or to try it all out in a fork, let me know
and I can help you out with what else you will need (which is pretty much
some style/css files and html templates).

** o-blog setup
*** local variables
- [ ] move to a properties basis to avoid forgetting to run this
#+begin_src emacs-lisp
      (setq o-blog-local-site "~/Sites/hyperq")
      (setq o-blog-out-dir "~/git/hyperq.github.io")
      (setq o-blog-local-url-index "http://127.0.0.1/~tonyday/hyperq/index.html")
#+end_src

#+results:
: http://127.0.0.1/~tonyday/hyperq/index.html

*** o-blog debugging
:PROPERTIES:
:PAGE:     debug.html
:TEMPLATE: debug.html
:SITEMAP:  f
:END:

  One method to debug o-blog production is to createa debug.html using the
  above properties.

  Another is to run the code below so you can test <lisp>snippets</lisp> on the fly

  - macro definitions copied from o-blog.el

    #+begin_src emacs-lisp
      (setq BLOG (ob-parse-blog-headers))
      (setq STATIC nil)
      (setq STATIC (append STATIC
                           (ob-parse-entries
                            (org-map-entries 'point-marker
                                             (ob:blog-static-filter BLOG)
                                             'file-with-archives))))
      (setq POSTS (ob-parse-entries
                   (org-map-entries 'point-marker
                                    (ob:blog-posts-filter BLOG)
                                    'file-with-archives)))
       
      (setq POST (ob-get-post-by-title POSTS "Signal Prediction"))
      (ob:post-id POST)
    #+end_src

    #+results:
    : 0

  - examples for on-the-fly debugging
    
    #+begin_src emacs-lisp
      (ob:path-to-root)
      (ob:blog-assets-dir BLOG)
      (ob:post-filepath POST)
      (ob:blog-url BLOG)
    #+end_src

  - naviation snippet
    #+begin_src emacs-lisp
      (setq ALL-POSTS POSTS)      
      (mapconcat (lambda (x) (format "<li><a href=\"%s/%s\">%s</a></li>"
                                (ob:path-to-root)
                                (ob:post-htmlfile (ob:get-last-post "blog" x))
                                (ob:post-title (ob:get-last-post "blog" x))
                                ))
                 '(0 1 2 3)  "")
    #+end_src


*** content
Most site content is located in the same place as it is developed. These
sections define summary and index pages that are created by o-blog and html
snippets used in page constrcution.

**** Articles by Category
  :PROPERTIES:
  :PAGE:     tags.html
  :TEMPLATE: blog_post-by-tags.html
  :END:

**** Navigation
  :PROPERTIES:
  :SNIPPET:  t
  :END:

#+begin_html
  <ul class="nav">
   <li>
     <a href="{lisp}(ob:path-to-root){/lisp}/about.html"><i>icon-user icon-white</i> About</a>
   </li>
   <li>
  <a class="caret-before" href="{lisp}(format "%s/%s" (ob:path-to-root) (ob:post-htmlfile (ob:get-last-post "blog" 0))){/lisp}">
       <i class="icon-coffee icon-white">
       </i>
       Blog
     </a>
  </li>
   <li class="dropdown">
 <a class="dropdown-toggle caret-after" data-toggle="dropdown" data-hover="dropdown" data-delay="2000">
 </a>
     <ul class="dropdown-menu">


{lisp}
     (mapconcat (lambda (x) (format "<li><a href=\"%s/%s\">%s</a></li>"
	   (ob:path-to-root)
	      (ob:post-htmlfile (ob:get-last-post "blog" x))
(ob:post-title (ob:get-last-post "blog" x))
))
'(0 1 2 3)  "")
{/lisp}

  </ul>

  </li>
  <li><a href="https://github.com/hyperq/hyperq"><i>icon-github icon-white</i>repo</a>

  </li>
  <li><a href="{lisp}(ob:path-to-root){/lisp}/tags.html"><i>icon-tags icon-white</i> Tags</a>

  </li>
  <li><a href="{lisp}(ob:path-to-root){/lisp}/archives.html"><i>icon-list icon-white</i> Archives</a>

  </li>
  <li><a href="{lisp}(ob:path-to-root){/lisp}/index.xml"><i>icon-rss icon-white</i> RSS</a>
  </li>
  <li><a href="http://scarcecapital.com"><i>icon-home icon-white</i> scarce,</a>
  </li>

  </ul>
#+end_html


**** Copyright
:PROPERTIES:
  :SNIPPET: t
  :END:

#+begin_html
Copyright <a href="http://github.com/hyperq">hyperq</a> 2013-2013

<p>Licensed under the Apache License, Version 2.0 (the "License");
you may not use this work except in compliance with the License.
You may obtain a copy of the License in the LICENSE file, or at:</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
#+end_html





